# -*- coding: utf-8 -*-
"""
âš¡ OLLAMA FAST ENGINE - Microservice Edition
============================================

LINJA OPTIKE - Zero overhead, zero fallback, zero kompleksitet.

NUK KA:
- Strategji komplekse
- Analiza keyword-esh  
- Fallback loops
- Modele 65GB
- Statistika/metadata tÃ« rÃ«nda

KA VETÃ‹M:
- 1 model (llama3.2:1b - 1.3GB)
- 1 thirrje HTTP
- 1 pÃ«rgjigje

Koha e pritshme: 3-8 sekonda (jo 40-90!)

Author: Clisonix Team
Version: 3.0.0 FAST
"""

import asyncio
import httpx
import logging
import os
from dataclasses import dataclass
from typing import Optional

logger = logging.getLogger("ollama_fast")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# KONFIGURIM - MINIMAL
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

IS_DOCKER = os.path.exists("/.dockerenv") or os.environ.get("DOCKER_ENV") == "1"
OLLAMA_HOST = os.environ.get("OLLAMA_HOST", "clisonix-06-ollama" if IS_DOCKER else "localhost")
if OLLAMA_HOST.startswith("http://"):
    OLLAMA_HOST = OLLAMA_HOST.replace("http://", "").split(":")[0]

OLLAMA_URL = os.environ.get("OLLAMA_URL", f"http://{OLLAMA_HOST}:11434")

# TIMEOUT: 30 sekonda max (jo 90!)
TIMEOUT = 30.0

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MODEL SELECTION - VETÃ‹M MODELE QÃ‹ FLASIN MIRÃ‹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# HEQUR: phi3:mini, clisonix-ocean:latest - flasin pÃ«rÃ§art nÃ« shqip
# MBAJTUR: llama3.1:8b - flet mirÃ« tÃ« gjitha gjuhÃ«t

DEFAULT_MODEL = "llama3.1:8b"  # Model i vetÃ«m i besueshÃ«m
FALLBACK_MODELS = ["llama3.1:8b", "clisonix-ocean:v2"]  # VetÃ«m modele tÃ« mÃ«dha

# ALBANIAN DETECTION
ALBANIAN_MARKERS = [
    "Ã«", "Ã§", "pÃ«rshÃ«ndetje", "mirÃ«mÃ«ngjes", "faleminderit", "shqip",
    "unÃ«", "jam", "jemi", "Ã«shtÃ«", "Ã§farÃ«", "pse", "kush", "ku", "kur",
    "mirÃ«dita", "tungjatjeta", "ndihmÃ«", "pyetje", "pÃ«rgjigje",
]

# SYSTEM PROMPT - Import from Master Prompt
SYSTEM_PROMPT: str
try:
    import sys
    from pathlib import Path
    modules_path = Path(__file__).parent.parent / "modules"
    if modules_path.exists():
        sys.path.insert(0, str(modules_path))
        from curiosity_ocean.master_prompt import CURIOSITY_OCEAN_COMPACT_PROMPT  # type: ignore
        SYSTEM_PROMPT = CURIOSITY_OCEAN_COMPACT_PROMPT
        logger.info("âš¡ Master prompt loaded (compact)")
    else:
        raise ImportError("modules path does not exist")
except Exception as e:
    logger.warning(f"âš ï¸ Could not import master_prompt: {e}")
    # Fallback - minimal prompt
    SYSTEM_PROMPT = """You are Curiosity Ocean, the AI of Clisonix Platform (clisonix.cloud).
Created by Ledjan Ahmati. Respond in the user's language. Be concise, accurate, helpful.
Never invent facts. Admit if unsure: "Nuk e di" / "I don't know"."""


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RESPONSE - MINIMAL
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class FastResponse:
    """PÃ«rgjigje minimale"""
    content: str
    model: str
    duration_ms: float
    success: bool


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# OLLAMA FAST ENGINE - LINJA OPTIKE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class OllamaFastEngine:
    """
    âš¡ OLLAMA FAST ENGINE
    
    Zero overhead. Zero fallback. Zero kompleksitet.
    VetÃ«m njÃ« thirrje direkte te Ollama.
    """
    
    def __init__(self, model: str = DEFAULT_MODEL):
        self.model = model
        self.url = OLLAMA_URL
        self._client: Optional[httpx.AsyncClient] = None
        self._model_verified = False
    
    async def _get_client(self) -> httpx.AsyncClient:
        """Lazy client - krijohet vetÃ«m njÃ« herÃ«"""
        if self._client is None or self._client.is_closed:
            self._client = httpx.AsyncClient(
                base_url=self.url,
                timeout=httpx.Timeout(TIMEOUT, connect=5.0)
            )
        return self._client
    
    async def verify_model(self) -> bool:
        """Verifiko modelin - thirret vetÃ«m 1 herÃ«"""
        if self._model_verified:
            return True
        
        try:
            client = await self._get_client()
            resp = await client.get("/api/tags", timeout=5.0)
            if resp.status_code == 200:
                models = [m["name"] for m in resp.json().get("models", [])]
                
                # Provo modelet nÃ« renditje prioriteti
                for model in FALLBACK_MODELS:
                    if model in models:
                        self.model = model
                        self._model_verified = True
                        logger.info(f"âš¡ Model {model} gati")
                        return True
                
                # PÃ«rdor Ã§do model tÃ« disponueshÃ«m
                if models:
                    self.model = models[0]
                    self._model_verified = True
                    logger.info(f"âš¡ Duke pÃ«rdorur {self.model}")
                    return True
        except Exception as e:
            logger.error(f"âŒ Ollama nuk u gjet: {e}")
        
        return False
    
    async def generate(self, prompt: str, system: str = None) -> FastResponse:
        """
        âš¡ GJENERO - NjÃ« thirrje, njÃ« pÃ«rgjigje
        
        Args:
            prompt: Pyetja
            system: System prompt (opsional)
        
        Returns:
            FastResponse
        """
        import time
        start = time.perf_counter()
        
        # Verifiko modelin (vetÃ«m herÃ«n e parÃ«)
        if not self._model_verified:
            await self.verify_model()
        
        try:
            client = await self._get_client()
            
            # THIRRJE E VETME - pa fallback, pa retry
            resp = await client.post(
                "/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "system": system or SYSTEM_PROMPT,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "num_predict": 1024,  # Max tokens - mjaftueshÃ«m
                    }
                }
            )
            
            elapsed_ms = (time.perf_counter() - start) * 1000
            
            if resp.status_code == 200:
                data = resp.json()
                return FastResponse(
                    content=data.get("response", ""),
                    model=self.model,
                    duration_ms=elapsed_ms,
                    success=True
                )
            
            logger.error(f"Ollama error: {resp.status_code}")
            return FastResponse(
                content=f"Gabim: Ollama ktheu {resp.status_code}",
                model=self.model,
                duration_ms=elapsed_ms,
                success=False
            )
            
        except httpx.TimeoutException:
            elapsed_ms = (time.perf_counter() - start) * 1000
            return FastResponse(
                content="Timeout - Ollama nuk u pÃ«rgjigj nÃ« kohÃ«",
                model=self.model,
                duration_ms=elapsed_ms,
                success=False
            )
        except Exception as e:
            elapsed_ms = (time.perf_counter() - start) * 1000
            logger.error(f"Generate error: {e}")
            return FastResponse(
                content=f"Gabim: {str(e)}",
                model=self.model,
                duration_ms=elapsed_ms,
                success=False
            )
    
    async def chat(self, message: str) -> FastResponse:
        """Alias pÃ«r generate - pÃ«r compatibility"""
        return await self.generate(message)
    
    async def close(self):
        """Mbyll klientin"""
        if self._client:
            await self._client.aclose()
            self._client = None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SINGLETON
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_engine: Optional[OllamaFastEngine] = None


def get_fast_engine() -> OllamaFastEngine:
    """Merr singleton"""
    global _engine
    if _engine is None:
        _engine = OllamaFastEngine()
    return _engine


async def fast_generate(prompt: str) -> str:
    """âš¡ Funksion i shpejtÃ« - njÃ« linjÃ«"""
    engine = get_fast_engine()
    response = await engine.generate(prompt)
    return response.content


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TEST
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    async def test():
        print("=" * 60)
        print("âš¡ OLLAMA FAST ENGINE TEST")
        print("=" * 60)
        
        engine = get_fast_engine()
        
        tests = [
            "PÃ«rshÃ«ndetje!",
            "What is 2+2?",
            "Ã‡farÃ« Ã«shtÃ« Clisonix?",
        ]
        
        for q in tests:
            print(f"\nğŸ“ {q}")
            r = await engine.generate(q)
            print(f"â±ï¸  {r.duration_ms:.0f}ms | âœ… {r.success}")
            print(f"ğŸ’¬ {r.content[:150]}...")
        
        await engine.close()
        print("\nâœ… Test complete!")
    
    asyncio.run(test())
