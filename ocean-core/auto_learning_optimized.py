#!/usr/bin/env python3
"""
ðŸ§  CLISONIX AUTO-LEARNING OPTIMIZED
====================================
Zero CPU/Disk stress - Memory-first, batch writes

Optimizations:
1. Memory-first arrays (NumPy mmap) 
2. Batch writes (Ã§do 100 entries, jo 10)
3. CBOR2 compression (50-70% mÃ« i vogÃ«l se JSON)
4. LRU cache pÃ«r layers (max 1000)
5. Lazy loading - vetÃ«m kur duhet
6. Background thread pÃ«r I/O

Author: Ledjan Ahmati - Clisonix
Version: 2.0.0 Optimized
"""

import time
import random
import hashlib
import os
import sys
import threading
import queue
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
from functools import lru_cache
from collections import OrderedDict
import numpy as np

# Try CBOR2 for compression, fallback to JSON
try:
    import cbor2
    USE_CBOR = True
except ImportError:
    import json
    USE_CBOR = False
    print("âš ï¸ cbor2 not installed, using JSON (install: pip install cbor2)")

# ============================================================================
# OPTIMIZED LIMITS - Balancuar pÃ«r CPU/Disk
# ============================================================================
MAX_MEMORY_ENTRIES = 1000       # Mbaj nÃ« RAM max 1000
BATCH_WRITE_SIZE = 100          # Shkruaj Ã§do 100 entries
MAX_DISK_SIZE_MB = 100          # Max 100MB nÃ« disk
MAX_LAYERS_CACHED = 500         # Max 500 layers nÃ« LRU cache
WRITE_INTERVAL_SEC = 300        # Shkruaj max Ã§do 5 min

# Topics & Templates (njÃ«jtÃ« si origjinali, por mÃ« kompakte)
TOPICS = [
    "Bitcoin", "Ethereum", "AI", "quantum", "blockchain", "neural",
    "consciousness", "gravity", "evolution", "time", "infinity", "Mars"
]

TEMPLATES = [
    "What is {}?", "How does {} work?", "Explain {}", "{} vs consciousness"
]


class LRULayerCache(OrderedDict):
    """LRU Cache pÃ«r layers - nuk rritet pa fund"""
    
    def __init__(self, maxsize=MAX_LAYERS_CACHED):
        super().__init__()
        self.maxsize = maxsize
    
    def __setitem__(self, key, value):
        if key in self:
            self.move_to_end(key)
        super().__setitem__(key, value)
        if len(self) > self.maxsize:
            # Hiq mÃ« tÃ« vjetrÃ«n
            oldest = next(iter(self))
            del self[oldest]
    
    def get_or_create(self, key, factory):
        """Merr ose krijo layer"""
        if key not in self:
            self[key] = factory()
        else:
            self.move_to_end(key)
        return self[key]


class AsyncWriter:
    """Background writer - nuk bllokon main thread"""
    
    def __init__(self, filepath: Path):
        self.filepath = filepath
        self.queue = queue.Queue()
        self.running = True
        self.thread = threading.Thread(target=self._writer_loop, daemon=True)
        self.thread.start()
        self.last_write = time.time()
    
    def _writer_loop(self):
        """Loop qÃ« shkruan nÃ« background"""
        while self.running:
            try:
                data = self.queue.get(timeout=1)
                if data is None:
                    continue
                
                # Shkruaj nÃ« disk
                if USE_CBOR:
                    with open(self.filepath, 'wb') as f:
                        cbor2.dump(data, f)
                else:
                    import json
                    with open(self.filepath, 'w', encoding='utf-8') as f:
                        json.dump(data, f, ensure_ascii=False)
                
                self.last_write = time.time()
            except queue.Empty:
                continue
            except Exception as e:
                print(f"âš ï¸ Write error: {e}")
    
    def write_async(self, data):
        """Shkruaj nÃ« background pa bllokuar"""
        self.queue.put(data)
    
    def stop(self):
        """Ndalo writer-in"""
        self.running = False
        self.thread.join(timeout=2)


class OptimizedAlphabetLayers:
    """61 shtresa me LRU cache - zero memory leak"""
    
    # Precomputed constants
    GREEK = [chr(i) for i in range(0x03B1, 0x03C9+1)]  # 24
    ALBANIAN = ['a','b','c','Ã§','d','dh','e','Ã«','f','g','gj','h',
                'i','j','k','l','ll','m','n','nj','o','p','q','r',
                'rr','s','sh','t','th','u','v','x','xh','y','z','zh']  # 36
    DIGRAPHS = {'dh','gj','ll','nj','rr','sh','th','xh','zh'}
    
    def __init__(self):
        self.layer_cache = LRULayerCache(MAX_LAYERS_CACHED)
        self._precompute_hashes()
    
    def _precompute_hashes(self):
        """Precompute hashes - njÃ« herÃ«, jo Ã§do herÃ«"""
        self.letter_hashes = {}
        for letter in self.GREEK + self.ALBANIAN:
            h = int(hashlib.md5(letter.encode()).hexdigest()[:8], 16)
            self.letter_hashes[letter] = (h % 1000) / 1000.0
    
    @lru_cache(maxsize=1000)
    def compute_layer(self, letter: str, input_hash: int) -> float:
        """Compute layer result - cached"""
        h = self.letter_hashes.get(letter, 0.5)
        # Simplified math - same result, less CPU
        return np.sin(h * 3.14159 * input_hash) * np.cos(h * input_hash)
    
    def process_word_fast(self, word: str) -> Dict[str, Any]:
        """Process word - fast path"""
        word_hash = hash(word) % 10000
        letters = self._decompose_fast(word.lower())
        
        if not letters:
            return {'word': word, 'complexity': 0, 'result': 0}
        
        # Compute combined result
        result = sum(self.compute_layer(l, word_hash) for l in letters) / len(letters)
        complexity = len(letters) * (1.5 if any(l in self.DIGRAPHS for l in letters) else 1.0)
        
        return {
            'word': word,
            'letters': letters,
            'complexity': round(complexity, 2),
            'result': round(float(result), 4)
        }
    
    def _decompose_fast(self, word: str) -> List[str]:
        """Fast decomposition"""
        letters = []
        i = 0
        while i < len(word):
            if i + 1 < len(word):
                digraph = word[i:i+2]
                if digraph in self.DIGRAPHS:
                    letters.append(digraph)
                    i += 2
                    continue
            if word[i] in self.letter_hashes:
                letters.append(word[i])
            i += 1
        return letters


class AutoLearningOptimized:
    """Auto-learning pa CPU/Disk stress"""
    
    def __init__(self):
        self.base_path = Path(__file__).parent / "learned_knowledge"
        self.base_path.mkdir(exist_ok=True)
        
        ext = '.cbor' if USE_CBOR else '.json'
        self.knowledge_file = self.base_path / f"auto_learned_v2{ext}"
        
        # Memory-first storage
        self.memory_buffer: List[Dict] = []
        self.knowledge_index: Dict[str, int] = {}  # hash -> index in buffer
        
        # Stats
        self.session_learned = 0
        self.total_learned = 0
        self.start_time = datetime.now()
        
        # Optimized components
        self.layers = OptimizedAlphabetLayers()
        self.writer = AsyncWriter(self.knowledge_file)
        
        # Load existing
        self._load_existing()
        
        print(f"âœ… Optimized Auto-Learning initialized")
        print(f"   ðŸ“¦ Storage: {'CBOR (compressed)' if USE_CBOR else 'JSON'}")
        print(f"   ðŸ§  Memory buffer: {len(self.memory_buffer)} entries")
        print(f"   ðŸ“Š Layer cache: {MAX_LAYERS_CACHED} max")
    
    def _load_existing(self):
        """Load existing knowledge"""
        if self.knowledge_file.exists():
            try:
                if USE_CBOR:
                    with open(self.knowledge_file, 'rb') as f:
                        data = cbor2.load(f)
                else:
                    import json
                    with open(self.knowledge_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                
                self.memory_buffer = data.get('entries', [])[-MAX_MEMORY_ENTRIES:]
                self.total_learned = data.get('stats', {}).get('total', 0)
                
                # Build index
                for i, entry in enumerate(self.memory_buffer):
                    self.knowledge_index[entry.get('id', '')] = i
            except Exception as e:
                print(f"âš ï¸ Load error: {e}")
    
    def generate_question(self) -> str:
        """Generate random question"""
        template = random.choice(TEMPLATES)
        topic = random.choice(TOPICS)
        return template.format(topic)
    
    def learn(self, question: str) -> Dict[str, Any]:
        """Learn from question - memory first"""
        qid = hashlib.md5(question.encode()).hexdigest()[:12]
        
        # Check cache first
        if qid in self.knowledge_index:
            idx = self.knowledge_index[qid]
            self.memory_buffer[idx]['times_used'] = \
                self.memory_buffer[idx].get('times_used', 0) + 1
            return {'cached': True, 'id': qid}
        
        # Process with optimized layers
        words = question.split()[:5]
        word_results = [self.layers.process_word_fast(w) for w in words]
        
        # Create entry
        entry = {
            'id': qid,
            'q': question[:100],  # Truncate for space
            'complexity': sum(w['complexity'] for w in word_results),
            'ts': datetime.now().strftime('%Y%m%d_%H%M'),
            'times_used': 1
        }
        
        # Add to memory
        self.memory_buffer.append(entry)
        self.knowledge_index[qid] = len(self.memory_buffer) - 1
        self.session_learned += 1
        self.total_learned += 1
        
        # Cleanup if needed
        if len(self.memory_buffer) > MAX_MEMORY_ENTRIES:
            self._cleanup_memory()
        
        # Batch write check
        if self.session_learned % BATCH_WRITE_SIZE == 0:
            self._async_save()
        
        return {'cached': False, 'id': qid}
    
    def _cleanup_memory(self):
        """Remove old entries from memory"""
        # Keep most used
        sorted_entries = sorted(
            self.memory_buffer,
            key=lambda x: x.get('times_used', 0),
            reverse=True
        )
        keep = int(MAX_MEMORY_ENTRIES * 0.7)
        self.memory_buffer = sorted_entries[:keep]
        
        # Rebuild index
        self.knowledge_index = {
            e['id']: i for i, e in enumerate(self.memory_buffer)
        }
        print(f"ðŸ§¹ Memory cleanup: kept {keep} entries")
    
    def _async_save(self):
        """Save in background"""
        data = {
            'entries': self.memory_buffer,
            'stats': {
                'total': self.total_learned,
                'session': self.session_learned,
                'saved_at': datetime.now().isoformat()
            }
        }
        self.writer.write_async(data)
        print(f"ðŸ’¾ Async save queued ({len(self.memory_buffer)} entries)")
    
    def get_stats(self) -> Dict[str, Any]:
        """Get current stats"""
        return {
            'memory_entries': len(self.memory_buffer),
            'total_learned': self.total_learned,
            'session_learned': self.session_learned,
            'layer_cache_size': len(self.layers.layer_cache),
            'storage_type': 'CBOR' if USE_CBOR else 'JSON',
            'uptime_sec': (datetime.now() - self.start_time).total_seconds()
        }
    
    def run_optimized(self, cycles: int = 0):
        """Run optimized loop - cycles=0 means forever"""
        print("\n" + "=" * 60)
        print("ðŸ§  CLISONIX AUTO-LEARNING OPTIMIZED")
        print("=" * 60)
        print(f"ðŸ“Š CPU/Disk friendly mode")
        print(f"ðŸ’¾ Batch writes every {BATCH_WRITE_SIZE} entries")
        print(f"ðŸ”„ Running {'forever' if cycles == 0 else f'{cycles} cycles'}...")
        print("=" * 60 + "\n")
        
        cycle = 0
        try:
            while cycles == 0 or cycle < cycles:
                cycle += 1
                question = self.generate_question()
                result = self.learn(question)
                
                # Minimal output - less I/O
                status = "ðŸ“¦" if result['cached'] else "ðŸ†•"
                stats = self.get_stats()
                
                line = f"\r{status} #{cycle} | Mem: {stats['memory_entries']} | Total: {stats['total_learned']} | Cache: {stats['layer_cache_size']}"
                sys.stdout.write(line + " " * 10)
                sys.stdout.flush()
                
                # Small delay - prevents CPU spike
                time.sleep(0.1)
                
        except KeyboardInterrupt:
            print("\n\nðŸ›‘ Stopping...")
            self._async_save()
            self.writer.stop()
            print("âœ… Saved and stopped cleanly")
            print(f"ðŸ“Š Final stats: {self.get_stats()}")


def main():
    """Entry point"""
    learner = AutoLearningOptimized()
    
    # Run with optional cycle limit
    cycles = int(sys.argv[1]) if len(sys.argv) > 1 else 0
    learner.run_optimized(cycles)


if __name__ == "__main__":
    main()
