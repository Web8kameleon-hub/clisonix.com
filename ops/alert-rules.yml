groups:
  - name: clisonix_api_alerts
    interval: 30s
    rules:
      # API AVAILABILITY & PERFORMANCE
      - alert: APIHighErrorRate
        expr: |
          (sum(rate(http_requests_total{status_code=~"5.."}[5m])) / sum(rate(http_requests_total[5m]))) > 0.05
        for: 2m
        labels:
          severity: critical
          service: api
        annotations:
          summary: "API Error Rate High ({{ $value | humanizePercentage }})"
          description: "API is returning 5xx errors at {{ $value | humanizePercentage }} rate"

      - alert: APILatencyHigh
        expr: |
          histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 1.0
        for: 5m
        labels:
          severity: warning
          service: api
        annotations:
          summary: "API Latency High ({{ $value | humanizeDuration }})"
          description: "95th percentile latency is {{ $value | humanizeDuration }}"

      - alert: APIDown
        expr: |
          up{job="api"} == 0
        for: 1m
        labels:
          severity: critical
          service: api
        annotations:
          summary: "API Service Down"
          description: "API service is not responding"

      # AI AGENT PERFORMANCE
      - alert: AIAgentHighLatency
        expr: |
          histogram_quantile(0.95, rate(ai_agent_duration_seconds_bucket[5m])) > 10.0
        for: 3m
        labels:
          severity: warning
          service: ai
        annotations:
          summary: "AI Agent {{ $labels.agent_type }} High Latency"
          description: "95th percentile latency is {{ $value | humanizeDuration }}"

      - alert: AIAgentHighErrorRate
        expr: |
          (sum(rate(ai_agent_executions_total{status="error"}[5m])) by (agent_type) / sum(rate(ai_agent_executions_total[5m])) by (agent_type)) > 0.1
        for: 2m
        labels:
          severity: warning
          service: ai
        annotations:
          summary: "AI Agent {{ $labels.agent_type }} High Error Rate"
          description: "Error rate is {{ $value | humanizePercentage }}"

      # DOCUMENT GENERATION
      - alert: DocumentGenerationErrors
        expr: |
          sum(rate(documents_generated_total{status="error"}[5m])) > 0
        for: 1m
        labels:
          severity: warning
          service: documents
        annotations:
          summary: "Document Generation Errors"
          description: "Documents are failing to generate"

      # DATABASE & CACHE
      - alert: PostgresDown
        expr: |
          up{job="postgres"} == 0
        for: 1m
        labels:
          severity: critical
          service: postgres
        annotations:
          summary: "PostgreSQL Service Down"
          description: "PostgreSQL database is not responding"

      - alert: RedisDown
        expr: |
          up{job="redis"} == 0
        for: 1m
        labels:
          severity: critical
          service: redis
        annotations:
          summary: "Redis Cache Down"
          description: "Redis cache is not responding"

      - alert: ElasticsearchDown
        expr: |
          up{job="elasticsearch"} == 0
        for: 1m
        labels:
          severity: critical
          service: elasticsearch
        annotations:
          summary: "Elasticsearch Down"
          description: "Elasticsearch service is not responding"

      # RESOURCE USAGE
      - alert: HighMemoryUsage
        expr: |
          (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) < 0.1
        for: 5m
        labels:
          severity: warning
          service: system
        annotations:
          summary: "High Memory Usage ({{ $value | humanizePercentage }} free)"
          description: "Memory usage is critically high"

      - alert: HighCPUUsage
        expr: |
          (1 - avg(rate(node_cpu_seconds_total{mode="idle"}[5m]))) > 0.8
        for: 5m
        labels:
          severity: warning
          service: system
        annotations:
          summary: "High CPU Usage ({{ $value | humanizePercentage }})"
          description: "CPU usage is high"

      - alert: DiskSpaceLow
        expr: |
          (node_filesystem_avail_bytes{fstype=~"ext4|xfs"} / node_filesystem_size_bytes) < 0.1
        for: 5m
        labels:
          severity: warning
          service: system
        annotations:
          summary: "Low Disk Space on {{ $labels.device }}"
          description: "Less than 10% disk space available"

      # ORCHESTRATION SERVICES
      - alert: OrchestratorDown
        expr: |
          up{job="orchestrator"} == 0
        for: 1m
        labels:
          severity: critical
          service: orchestrator
        annotations:
          summary: "Orchestrator Service Down"
          description: "Mesh orchestrator is not responding"

      - alert: SaaSServiceDown
        expr: |
          up{tier="saas"} == 0
        for: 2m
        labels:
          severity: critical
          service: saas
        annotations:
          summary: "{{ $labels.service | toUpper }} Service Down"
          description: "{{ $labels.service }} is not responding"

  - name: clisonix_business_alerts
    interval: 1m
    rules:
      # SLA MONITORING
      - alert: APISLAViolation
        expr: |
          (sum(rate(http_requests_total{status_code=~"2.."}[1h])) / sum(rate(http_requests_total[1h]))) < 0.99
        for: 5m
        labels:
          severity: critical
          service: sla
        annotations:
          summary: "API SLA Violation - Availability Below 99%"
          description: "Availability is {{ $value | humanizePercentage }}"

      - alert: HighValidationErrorRate
        expr: |
          sum(rate(api_validation_errors_total[5m])) > 1
        for: 2m
        labels:
          severity: warning
          service: api
        annotations:
          summary: "High Validation Error Rate"
          description: "Validation errors detected in API requests"

      # QUEUE & BACKGROUND JOBS
      - alert: HighQueueDepth
        expr: |
          queue_depth > 1000
        for: 5m
        labels:
          severity: warning
          service: processing
        annotations:
          summary: "Processing Queue Backlog"
          description: "Queue {{ $labels.queue_name }} has {{ $value }} items"

      - alert: BackgroundJobsFailing
        expr: |
          increase(background_jobs_running[5m]) == 0
        for: 10m
        labels:
          severity: warning
          service: background
        annotations:
          summary: "No Background Jobs Processing"
          description: "Background job processing may be stalled"

  - name: clisonix_victoria_metrics
    interval: 30s
    rules:
      - alert: VictoriaMetricsDown
        expr: |
          up{job="victoria-metrics"} == 0
        for: 1m
        labels:
          severity: critical
          service: monitoring
        annotations:
          summary: "VictoriaMetrics Down"
          description: "VictoriaMetrics service is not responding"

      - alert: VictoriaMetricsHighChurnRate
        expr: |
          vm_metrics_with_high_cardinality > 0
        for: 5m
        labels:
          severity: warning
          service: monitoring
        annotations:
          summary: "High Metric Cardinality Detected"
          description: "VictoriaMetrics is tracking too many unique time series"

      - alert: PrometheusRemoteWriteFailing
        expr: |
          rate(prometheus_remote_storage_samples_dropped_total[5m]) > 0
        for: 5m
        labels:
          severity: warning
          service: monitoring
        annotations:
          summary: "Prometheus Remote Write Failing"
          description: "Samples are being dropped in remote write to VictoriaMetrics"

  # ============================================================================
  # DATABASE ALERTS
  # ============================================================================
  - name: postgres_alerts
    interval: 30s
    rules:
      - alert: PostgreSQLDown
        expr: |
          up{job="postgres"} == 0
        for: 1m
        labels:
          severity: critical
          service: postgres
        annotations:
          summary: "PostgreSQL Exporter Down"
          description: "Cannot scrape PostgreSQL metrics"

      - alert: PostgreSQLTooManyConnections
        expr: |
          pg_stat_activity_count > 80
        for: 5m
        labels:
          severity: warning
          service: postgres
        annotations:
          summary: "PostgreSQL High Connection Count ({{ $value }})"
          description: "PostgreSQL has {{ $value }} active connections (threshold: 80)"

      - alert: PostgreSQLSlowQueries
        expr: |
          pg_slow_queries > 10
        for: 5m
        labels:
          severity: warning
          service: postgres
        annotations:
          summary: "PostgreSQL Slow Queries Detected"
          description: "{{ $value }} slow queries detected in last 5 minutes"

      - alert: PostgreSQLReplicationLag
        expr: |
          pg_replication_lag_seconds > 10
        for: 5m
        labels:
          severity: warning
          service: postgres
        annotations:
          summary: "PostgreSQL Replication Lag High"
          description: "Replication lag is {{ $value }} seconds"

      - alert: PostgreSQLDiskUsageHigh
        expr: |
          pg_database_size_bytes / 1073741824 > 50
        for: 10m
        labels:
          severity: warning
          service: postgres
        annotations:
          summary: "PostgreSQL Disk Usage High"
          description: "Database size is {{ $value | humanize }}GB (threshold: 50GB)"

  # ============================================================================
  # CACHE ALERTS
  # ============================================================================
  - name: redis_alerts
    interval: 30s
    rules:
      - alert: RedisDown
        expr: |
          up{job="redis"} == 0
        for: 1m
        labels:
          severity: critical
          service: redis
        annotations:
          summary: "Redis Exporter Down"
          description: "Cannot scrape Redis metrics"

      - alert: RedisHighMemoryUsage
        expr: |
          redis_memory_used_bytes / redis_memory_max_bytes > 0.85
        for: 5m
        labels:
          severity: warning
          service: redis
        annotations:
          summary: "Redis Memory Usage High ({{ $value | humanizePercentage }})"
          description: "Redis memory usage is {{ $value | humanizePercentage }} of max"

      - alert: RedisEvictionsHigh
        expr: |
          rate(redis_evicted_keys_total[5m]) > 100
        for: 5m
        labels:
          severity: critical
          service: redis
        annotations:
          summary: "Redis Evicting Keys ({{ $value }} keys/sec)"
          description: "Redis is evicting {{ $value | humanize }} keys per second"

      - alert: RedisLowHitRate
        expr: |
          redis_keyspace_hits_total / (redis_keyspace_hits_total + redis_keyspace_misses_total) < 0.8
        for: 10m
        labels:
          severity: warning
          service: redis
        annotations:
          summary: "Redis Cache Hit Rate Low ({{ $value | humanizePercentage }})"
          description: "Redis hit rate is {{ $value | humanizePercentage }} (threshold: 80%)"

      - alert: RedisConnectedClients
        expr: |
          redis_connected_clients > 500
        for: 5m
        labels:
          severity: warning
          service: redis
        annotations:
          summary: "Redis High Client Connections ({{ $value }})"
          description: "Redis has {{ $value }} connected clients"

