"""
CLISONIX CONTENT FACTORY - Main Orchestrator
=============================================

The complete content production pipeline:

    News/Law Watcher â†’ BLERINA â†’ EAP Layer â†’ Trinity â†’ Ocean â†’ Quality Selector â†’ CLX-Publisher

Components:
- News/Law Watcher: Ingestion module for news, laws, reports
- BLERINA: Gap-Detector & Conceptual Reconstruction
- EAP Layer: Evresi â†’ Analysi â†’ Proposi (3-stage processing)
- Trinity: ALBA â†’ ALBI â†’ JONA (data collection, analysis, synthesis)
- Ocean: Writer AI for final content
- Quality Selector: Filters documents by quality score
- CLX-Publisher: Auto-publish to multiple platforms

Production Capacity:
- Manual Mode: 5-15 docs/day
- Watcher Mode: 30-60 docs/day
- Continuous Mode: 100-200 docs/day
- Full Autopilot: 300-500 docs/day
- Publication Rate: 10-20 selected articles/day

Author: Ledjan Ahmati (CEO, ABA GmbH)
System: Clisonix Cloud
"""

from __future__ import annotations

import asyncio
import hashlib
import json
import logging
import os
from dataclasses import dataclass, field
from datetime import datetime, timezone
from enum import Enum
from typing import Any, Dict, List, Optional

# Import components with type ignore for optional dependencies
try:
    from blerina_core import BlerinaCore, DocumentType, get_blerina
except ImportError:
    BlerinaCore = None  # type: ignore[assignment,misc]
    DocumentType = None  # type: ignore[assignment,misc]
    get_blerina = None  # type: ignore[assignment]
    print("âš ï¸ blerina_core not found")

try:
    from eap_layer import EAPDocument, EAPLayer, get_eap
except ImportError:
    EAPLayer = None  # type: ignore[assignment,misc]
    EAPDocument = None  # type: ignore[assignment,misc]
    get_eap = None  # type: ignore[assignment]
    print("âš ï¸ eap_layer not found")

try:
    from clx_publisher import CLXPublisher, get_publisher
except ImportError:
    CLXPublisher = None  # type: ignore[assignment,misc]
    get_publisher = None  # type: ignore[assignment]
    print("âš ï¸ clx_publisher not found")

# Try httpx for external API calls
try:
    import httpx
except ImportError:
    httpx = None  # type: ignore[assignment]

logger = logging.getLogger("content_factory")
logger.setLevel(logging.INFO)

# Console handler
if not logger.handlers:
    handler = logging.StreamHandler()
    handler.setFormatter(logging.Formatter('%(asctime)s | %(levelname)s | %(message)s'))
    logger.addHandler(handler)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ENUMS & CONFIGURATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class FactoryMode(str, Enum):
    """Operating modes"""
    MANUAL = "manual"           # 5-15 docs/day, user triggers
    WATCHER = "watcher"         # 30-60 docs/day, auto from news
    CONTINUOUS = "continuous"   # 100-200 docs/day, Blerina continuous
    AUTOPILOT = "autopilot"     # 300-500 docs/day, full auto


class SourceType(str, Enum):
    """Source types for ingestion"""
    NEWS = "news"
    LAW = "law"
    POLICY = "policy"
    RESEARCH = "research"
    REPORT = "report"
    MANUAL = "manual"


@dataclass
class FactoryConfig:
    """Content Factory configuration"""
    mode: FactoryMode = FactoryMode.MANUAL
    quality_threshold: float = 0.7
    max_docs_per_day: int = 20
    max_publish_per_day: int = 10
    enabled_platforms: List[str] = field(default_factory=lambda: ["clisonix"])
    ollama_url: str = "http://localhost:11434"
    model: str = "llama3.1:8b"
    output_dir: str = "output/content_factory"
    auto_publish: bool = False
    watch_sources: List[Dict] = field(default_factory=list)


@dataclass
class SourceItem:
    """An item from news/law watcher"""
    id: str
    source_type: SourceType
    title: str
    content: str
    url: Optional[str] = None
    date: Optional[str] = None
    author: Optional[str] = None
    tags: List[str] = field(default_factory=list)
    metadata: Dict = field(default_factory=dict)


@dataclass
class FactoryResult:
    """Result from content factory processing"""
    id: str
    source_id: str
    blerina_signal_id: Optional[str] = None
    eap_document_id: Optional[str] = None
    gaps_found: int = 0
    discontinuity_level: Optional[str] = None
    quality_score: float = 0.0
    published: bool = False
    publish_platforms: List[str] = field(default_factory=list)
    publish_urls: List[str] = field(default_factory=list)
    processing_time_ms: float = 0.0
    status: str = "pending"
    error: Optional[str] = None
    timestamp: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())
    
    def to_dict(self) -> Dict:
        return {
            "id": self.id,
            "source_id": self.source_id,
            "blerina_signal_id": self.blerina_signal_id,
            "eap_document_id": self.eap_document_id,
            "gaps_found": self.gaps_found,
            "discontinuity_level": self.discontinuity_level,
            "quality_score": self.quality_score,
            "published": self.published,
            "publish_platforms": self.publish_platforms,
            "publish_urls": self.publish_urls,
            "processing_time_ms": self.processing_time_ms,
            "status": self.status,
            "error": self.error,
            "timestamp": self.timestamp
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# NEWS & LAW WATCHER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class NewsLawWatcher:
    """
    Ingestion module for news, laws, and reports.
    
    Monitors sources and feeds content into the pipeline.
    """
    
    def __init__(self) -> None:
        self._sources: List[Dict] = []
        self._items_ingested: int = 0
        self._last_check: Optional[str] = None
    
    def add_source(
        self,
        name: str,
        source_type: SourceType,
        url: Optional[str] = None,
        api_key: Optional[str] = None,
        check_interval: int = 3600
    ) -> None:
        """Add a source to watch"""
        self._sources.append({
            "name": name,
            "type": source_type.value,
            "url": url,
            "api_key": api_key,
            "interval": check_interval,
            "last_check": None,
            "items_found": 0
        })
        logger.info(f"ğŸ“¡ Added source: {name} ({source_type.value})")
    
    async def check_sources(self) -> List[SourceItem]:
        """Check all sources for new items"""
        items = []
        
        for source in self._sources:
            try:
                source_items = await self._check_source(source)
                items.extend(source_items)
                source["last_check"] = datetime.now(timezone.utc).isoformat()
                source["items_found"] += len(source_items)
            except Exception as e:
                logger.error(f"Error checking source {source['name']}: {e}")
        
        self._items_ingested += len(items)
        self._last_check = datetime.now(timezone.utc).isoformat()
        
        return items
    
    async def _check_source(self, source: Dict) -> List[SourceItem]:
        """Check individual source - REAL IMPLEMENTATION"""
        import aiohttp
        
        items = []
        source_type = source.get("type", "")
        url = source.get("url")
        api_key = source.get("api_key")
        
        if not url:
            return items
        
        try:
            async with aiohttp.ClientSession() as session:
                headers = {"User-Agent": "Clisonix-Bot/1.0"}
                if api_key:
                    headers["Authorization"] = f"Bearer {api_key}"
                
                # RSS/News feeds
                if source_type in ["news", "rss", "feed"]:
                    items = await self._fetch_rss_feed(session, url, source)
                
                # REST APIs (JSON)
                elif source_type in ["api", "rest", "json"]:
                    items = await self._fetch_api_data(session, url, headers, source)
                
                # Web scraping for laws/regulations
                elif source_type in ["law", "regulation", "government"]:
                    items = await self._scrape_legal_page(session, url, headers, source)
                
                # Research/Academic (ArXiv, PubMed style)
                elif source_type in ["research", "academic", "arxiv", "pubmed"]:
                    items = await self._fetch_research_api(session, url, headers, source)
                
                # Generic web page
                else:
                    items = await self._scrape_web_page(session, url, headers, source)
                    
        except Exception as e:
            logger.error(f"Source check failed for {source.get('name')}: {e}")
        
        return items
    
    async def _fetch_rss_feed(self, session, url: str, source: Dict) -> List[SourceItem]:
        """Fetch and parse RSS/Atom feed"""
        items = []
        try:
            async with session.get(url, timeout=aiohttp.ClientTimeout(total=30)) as resp:
                if resp.status == 200:
                    content = await resp.text()
                    # Simple RSS parsing (production would use feedparser)
                    import re
                    # Extract items from RSS
                    item_pattern = r'<item>(.*?)</item>'
                    title_pattern = r'<title>(?:<!\[CDATA\[)?(.*?)(?:\]\]>)?</title>'
                    desc_pattern = r'<description>(?:<!\[CDATA\[)?(.*?)(?:\]\]>)?</description>'
                    link_pattern = r'<link>(.*?)</link>'
                    
                    for match in re.findall(item_pattern, content, re.DOTALL)[:10]:
                        title_match = re.search(title_pattern, match)
                        desc_match = re.search(desc_pattern, match)
                        link_match = re.search(link_pattern, match)
                        
                        if title_match:
                            item_id = hashlib.md5(match[:100].encode()).hexdigest()[:10]
                            items.append(SourceItem(
                                id=f"rss_{item_id}",
                                source_type=SourceType.NEWS,
                                title=title_match.group(1).strip(),
                                content=desc_match.group(1).strip() if desc_match else "",
                                url=link_match.group(1).strip() if link_match else url,
                                date=datetime.now(timezone.utc).isoformat()[:10],
                                tags=[source.get("name", "news")]
                            ))
        except Exception as e:
            logger.warning(f"RSS fetch failed: {e}")
        return items
    
    async def _fetch_api_data(self, session, url: str, headers: Dict, source: Dict) -> List[SourceItem]:
        """Fetch data from JSON API"""
        items = []
        try:
            async with session.get(url, headers=headers, timeout=aiohttp.ClientTimeout(total=30)) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    # Handle common API structures
                    results = data if isinstance(data, list) else data.get("results", data.get("items", data.get("data", [])))
                    
                    for item in results[:10]:
                        if isinstance(item, dict):
                            item_id = hashlib.md5(str(item).encode()).hexdigest()[:10]
                            items.append(SourceItem(
                                id=f"api_{item_id}",
                                source_type=SourceType.API,
                                title=item.get("title", item.get("name", "Untitled")),
                                content=item.get("description", item.get("content", item.get("body", str(item)))),
                                url=item.get("url", item.get("link", url)),
                                date=datetime.now(timezone.utc).isoformat()[:10],
                                tags=[source.get("name", "api")]
                            ))
        except Exception as e:
            logger.warning(f"API fetch failed: {e}")
        return items
    
    async def _scrape_legal_page(self, session, url: str, headers: Dict, source: Dict) -> List[SourceItem]:
        """Scrape legal/government pages"""
        items = []
        try:
            async with session.get(url, headers=headers, timeout=aiohttp.ClientTimeout(total=30)) as resp:
                if resp.status == 200:
                    content = await resp.text()
                    # Extract title and main content
                    import re
                    title_match = re.search(r'<title>(.*?)</title>', content, re.IGNORECASE)
                    # Remove HTML tags for content
                    text_content = re.sub(r'<[^>]+>', ' ', content)
                    text_content = re.sub(r'\s+', ' ', text_content).strip()
                    
                    if len(text_content) > 200:
                        item_id = hashlib.md5(url.encode()).hexdigest()[:10]
                        items.append(SourceItem(
                            id=f"law_{item_id}",
                            source_type=SourceType.LAW,
                            title=title_match.group(1).strip() if title_match else "Legal Document",
                            content=text_content[:5000],
                            url=url,
                            date=datetime.now(timezone.utc).isoformat()[:10],
                            tags=[source.get("name", "legal"), "regulation"]
                        ))
        except Exception as e:
            logger.warning(f"Legal page scrape failed: {e}")
        return items
    
    async def _fetch_research_api(self, session, url: str, headers: Dict, source: Dict) -> List[SourceItem]:
        """Fetch from research APIs (ArXiv, PubMed, etc.)"""
        items = []
        source_name = source.get("name", "").lower()
        
        try:
            # ArXiv API
            if "arxiv" in source_name or "arxiv" in url:
                arxiv_url = "http://export.arxiv.org/api/query?search_query=all:AI+healthcare&max_results=5"
                async with session.get(arxiv_url, timeout=aiohttp.ClientTimeout(total=30)) as resp:
                    if resp.status == 200:
                        content = await resp.text()
                        import re
                        entries = re.findall(r'<entry>(.*?)</entry>', content, re.DOTALL)
                        for entry in entries[:5]:
                            title_match = re.search(r'<title>(.*?)</title>', entry, re.DOTALL)
                            summary_match = re.search(r'<summary>(.*?)</summary>', entry, re.DOTALL)
                            if title_match:
                                item_id = hashlib.md5(entry[:100].encode()).hexdigest()[:10]
                                items.append(SourceItem(
                                    id=f"arxiv_{item_id}",
                                    source_type=SourceType.RESEARCH,
                                    title=title_match.group(1).strip().replace('\n', ' '),
                                    content=summary_match.group(1).strip().replace('\n', ' ') if summary_match else "",
                                    url=url,
                                    date=datetime.now(timezone.utc).isoformat()[:10],
                                    tags=["arxiv", "research", "AI"]
                                ))
            
            # Generic research API
            else:
                items = await self._fetch_api_data(session, url, headers, source)
                
        except Exception as e:
            logger.warning(f"Research API fetch failed: {e}")
        return items
    
    async def _scrape_web_page(self, session, url: str, headers: Dict, source: Dict) -> List[SourceItem]:
        """Generic web page scraping"""
        items = []
        try:
            async with session.get(url, headers=headers, timeout=aiohttp.ClientTimeout(total=30)) as resp:
                if resp.status == 200:
                    content = await resp.text()
                    import re
                    title_match = re.search(r'<title>(.*?)</title>', content, re.IGNORECASE)
                    # Extract main content (simplified)
                    text_content = re.sub(r'<script[^>]*>.*?</script>', '', content, flags=re.DOTALL | re.IGNORECASE)
                    text_content = re.sub(r'<style[^>]*>.*?</style>', '', text_content, flags=re.DOTALL | re.IGNORECASE)
                    text_content = re.sub(r'<[^>]+>', ' ', text_content)
                    text_content = re.sub(r'\s+', ' ', text_content).strip()
                    
                    if len(text_content) > 200:
                        item_id = hashlib.md5(url.encode()).hexdigest()[:10]
                        items.append(SourceItem(
                            id=f"web_{item_id}",
                            source_type=SourceType.MANUAL,
                            title=title_match.group(1).strip() if title_match else source.get("name", "Web Content"),
                            content=text_content[:5000],
                            url=url,
                            date=datetime.now(timezone.utc).isoformat()[:10],
                            tags=[source.get("name", "web")]
                        ))
        except Exception as e:
            logger.warning(f"Web scrape failed: {e}")
        return items
    
    def create_manual_item(
        self,
        title: str,
        content: str,
        source_type: SourceType = SourceType.MANUAL,
        url: Optional[str] = None,
        tags: Optional[List[str]] = None
    ) -> SourceItem:
        """Create a manual item for processing"""
        item_id = f"manual_{hashlib.md5(content[:100].encode()).hexdigest()[:10]}"
        
        return SourceItem(
            id=item_id,
            source_type=source_type,
            title=title,
            content=content,
            url=url,
            date=datetime.now(timezone.utc).isoformat()[:10],
            tags=tags or []
        )
    
    def get_stats(self) -> Dict:
        """Get watcher statistics"""
        return {
            "sources_count": len(self._sources),
            "items_ingested": self._items_ingested,
            "last_check": self._last_check,
            "sources": [
                {
                    "name": s["name"],
                    "type": s["type"],
                    "items_found": s["items_found"],
                    "last_check": s["last_check"]
                }
                for s in self._sources
            ]
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# QUALITY SELECTOR
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class QualitySelector:
    """
    Filters and ranks documents by quality.
    
    Selects the best documents for publication.
    """
    
    def __init__(self, threshold: float = 0.7, max_per_day: int = 20):
        self.threshold = threshold
        self.max_per_day = max_per_day
        self._today_count = 0
        self._today_date = datetime.now(timezone.utc).strftime("%Y-%m-%d")
        self._selected: List[str] = []
        self._rejected: List[str] = []
    
    def assess(self, doc: Any) -> Dict[str, Any]:  # doc is EAPDocument
        """Assess document quality"""
        score = 0.0
        criteria = {}
        
        # Content completeness (40%)
        if doc.evresi and len(doc.evresi.facts) >= 3:
            score += 0.15
            criteria["facts"] = "âœ“"
        else:
            criteria["facts"] = "âœ—"
        
        if doc.analysi and len(doc.analysi.structural_gaps) >= 2:
            score += 0.15
            criteria["gaps"] = "âœ“"
        else:
            criteria["gaps"] = "âœ—"
        
        if doc.proposi and doc.proposi.paradigm_name:
            score += 0.10
            criteria["proposal"] = "âœ“"
        else:
            criteria["proposal"] = "âœ—"
        
        # Depth of analysis (30%)
        if doc.analysi and len(doc.analysi.risks) >= 2:
            score += 0.10
            criteria["risks"] = "âœ“"
        else:
            criteria["risks"] = "âœ—"
        
        if doc.proposi and len(doc.proposi.principles) >= 3:
            score += 0.10
            criteria["principles"] = "âœ“"
        else:
            criteria["principles"] = "âœ—"
        
        if doc.proposi and len(doc.proposi.implementation_steps) >= 3:
            score += 0.10
            criteria["implementation"] = "âœ“"
        else:
            criteria["implementation"] = "âœ—"
        
        # Presentation (20%)
        if doc.executive_summary and len(doc.executive_summary) >= 100:
            score += 0.10
            criteria["summary"] = "âœ“"
        else:
            criteria["summary"] = "âœ—"
        
        if doc.tags and len(doc.tags) >= 3:
            score += 0.05
            criteria["tags"] = "âœ“"
        else:
            criteria["tags"] = "âœ—"
        
        if doc.title and len(doc.title) >= 10:
            score += 0.05
            criteria["title"] = "âœ“"
        else:
            criteria["title"] = "âœ—"
        
        # Severity bonus (10%)
        if doc.analysi and doc.analysi.severity_level in ["critical", "major"]:
            score += 0.10
            criteria["urgency"] = "âœ“ (priority)"
        else:
            criteria["urgency"] = "normal"
        
        return {
            "score": round(score, 2),
            "passed": score >= self.threshold,
            "criteria": criteria,
            "threshold": self.threshold
        }
    
    def select(self, documents: List[Any]) -> List[Any]:  # List[EAPDocument]
        """Select best documents for publication"""
        # Reset daily count if new day
        today = datetime.now(timezone.utc).strftime("%Y-%m-%d")
        if today != self._today_date:
            self._today_count = 0
            self._today_date = today
            self._selected = []
            self._rejected = []
        
        # Assess all documents
        assessed = []
        for doc in documents:
            quality = self.assess(doc)
            if quality["passed"]:
                assessed.append((doc, quality["score"]))
            else:
                self._rejected.append(doc.id)
        
        # Sort by score and select top N
        assessed.sort(key=lambda x: x[1], reverse=True)
        remaining_slots = self.max_per_day - self._today_count
        
        selected = []
        for doc, score in assessed[:remaining_slots]:
            selected.append(doc)
            self._selected.append(doc.id)
            self._today_count += 1
        
        logger.info(f"ğŸ“Š Selected {len(selected)}/{len(documents)} documents (threshold: {self.threshold})")
        return selected
    
    def get_stats(self) -> Dict:
        """Get selector statistics"""
        return {
            "threshold": self.threshold,
            "max_per_day": self.max_per_day,
            "today_count": self._today_count,
            "selected_ids": self._selected[-20:],
            "rejected_count": len(self._rejected)
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# OCEAN WRITER (Integration with Ocean AI)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class OceanWriter:
    """
    Integration with Ocean AI for final content writing.
    
    Takes EAP documents and enhances them with Ocean's writing style.
    """
    
    def __init__(self, ollama_url: str = "http://localhost:11434"):
        self.ollama_url = ollama_url
        self.model = "llama3.1:8b"
        self._documents_written = 0
    
    async def enhance_document(self, doc: Any) -> Any:  # EAPDocument
        """Enhance document with Ocean's writing style"""
        if not httpx:
            return doc
        
        # Enhance executive summary
        enhanced_summary = await self._enhance_text(
            doc.executive_summary,
            "Make this executive summary more compelling and professional while keeping it concise."
        )
        
        if enhanced_summary:
            doc.executive_summary = enhanced_summary
        
        self._documents_written += 1
        return doc
    
    async def _enhance_text(self, text: str, instruction: str) -> Optional[str]:
        """Enhance text using Ocean/LLM"""
        if not httpx:
            return None
        
        prompt = f"""{instruction}

Original text:
{text}

Enhanced text:"""
        
        try:
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(
                    f"{self.ollama_url}/api/generate",
                    json={
                        "model": self.model,
                        "prompt": prompt,
                        "stream": False
                    }
                )
                
                if response.status_code == 200:
                    result = response.json()
                    return result.get("response", text)
        except Exception as e:
            logger.error(f"Ocean enhancement failed: {e}")
        
        return None
    
    def get_stats(self) -> Dict:
        """Get writer statistics"""
        return {
            "documents_written": self._documents_written,
            "model": self.model
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONTENT FACTORY - MAIN ORCHESTRATOR
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class ContentFactory:
    """
    Clisonix Content Factory - Main Orchestrator
    
    Orchestrates the complete content production pipeline:
    
    1. News/Law Watcher â†’ Ingests sources
    2. BLERINA â†’ Detects gaps
    3. EAP Layer â†’ Processes through Evresi â†’ Analysi â†’ Proposi
    4. Trinity â†’ (Optional) Advanced analysis
    5. Ocean Writer â†’ Enhances content
    6. Quality Selector â†’ Filters by quality
    7. CLX-Publisher â†’ Publishes to platforms
    
    Usage:
        factory = ContentFactory()
        
        # Manual mode - process single document
        result = await factory.process(content, title="My Analysis", topic="governance")
        
        # Watcher mode - auto-process from sources
        factory.add_source("EU Laws", SourceType.LAW, url="...")
        results = await factory.run_watcher_cycle()
        
        # Continuous mode - run indefinitely
        await factory.run_continuous()
    """
    
    def __init__(self, config: Optional[FactoryConfig] = None) -> None:
        self.config = config or FactoryConfig()
        
        # Initialize components
        self.watcher = NewsLawWatcher()
        self.blerina = get_blerina() if get_blerina is not None else None
        self.eap = get_eap() if get_eap is not None else None
        self.publisher = get_publisher() if get_publisher is not None else None
        self.selector = QualitySelector(
            threshold=self.config.quality_threshold,
            max_per_day=self.config.max_publish_per_day
        )
        self.ocean = OceanWriter(self.config.ollama_url)
        
        # State
        self._running = False
        self._results: List[FactoryResult] = []
        self._documents_produced: List[Any] = []  # EAPDocument when available
        
        # Ensure output directory
        os.makedirs(self.config.output_dir, exist_ok=True)
        
        logger.info(f"ğŸ­ Content Factory initialized (mode: {self.config.mode.value})")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # MAIN PROCESSING
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    async def process(
        self,
        content: str,
        title: str = "Analysis",
        topic: str = "governance",
        source_type: SourceType = SourceType.MANUAL,
        url: Optional[str] = None,
        tags: Optional[List[str]] = None,
        auto_publish: Optional[bool] = None
    ) -> FactoryResult:
        """
        Process single content through the full pipeline.
        
        Args:
            content: Document content to analyze
            title: Document title
            topic: Topic for analysis
            source_type: Type of source
            url: Source URL if any
            tags: Tags for the document
            auto_publish: Override config auto_publish
            
        Returns:
            FactoryResult with all processing details
        """
        import time
        start_time = time.time()
        
        result_id = f"factory_{hashlib.md5(content[:100].encode()).hexdigest()[:12]}"
        source_id = f"source_{hashlib.md5(content[:50].encode()).hexdigest()[:8]}"
        
        result = FactoryResult(
            id=result_id,
            source_id=source_id,
            status="processing"
        )
        
        logger.info(f"ğŸ”„ Processing: {title[:50]}...")
        
        try:
            # Step 1: BLERINA - Gap Detection
            if self.blerina:
                logger.info("  â””â”€ Running BLERINA gap detection...")
                gaps = await self.blerina.extract_gaps(content, DocumentType.POLICY)
                analysis = await self.blerina.detect_discontinuity([content], topic)
                signal = self.blerina.generate_signal(gaps, analysis)
                
                result.blerina_signal_id = signal.id
                result.gaps_found = len(gaps)
                result.discontinuity_level = analysis.discontinuity_level.value
            else:
                signal = None
            
            # Step 2: EAP Layer - 3-Stage Processing
            if self.eap:
                logger.info("  â””â”€ Running EAP Layer (Evresi â†’ Analysi â†’ Proposi)...")
                
                if signal and signal.eap_ready:
                    eap_doc = await self.eap.process(signal)
                else:
                    eap_doc = await self.eap.process_document(content, topic)
                
                # Add title and tags
                eap_doc.title = title
                if tags:
                    eap_doc.tags = list(set(eap_doc.tags + tags))
                
                result.eap_document_id = eap_doc.id
            else:
                logger.warning("  â””â”€ EAP Layer not available, creating basic document")
                eap_doc = await self._create_basic_document(content, title, topic, tags or [])
                result.eap_document_id = eap_doc.id if eap_doc else None
            
            if eap_doc is None:
                raise ValueError("Failed to create EAP document")
            
            # Step 3: Ocean Writer - Enhance content
            logger.info("  â””â”€ Running Ocean Writer enhancement...")
            eap_doc = await self.ocean.enhance_document(eap_doc)
            
            # Step 4: Quality Selection
            logger.info("  â””â”€ Running Quality Selector...")
            quality = self.selector.assess(eap_doc)
            result.quality_score = quality["score"]
            
            # Store document
            self._documents_produced.append(eap_doc)
            
            # Save to file
            self._save_document(eap_doc)
            
            # Step 5: Publishing (if enabled)
            should_publish = auto_publish if auto_publish is not None else self.config.auto_publish
            
            if should_publish and quality["passed"] and self.publisher:
                logger.info("  â””â”€ Publishing to platforms...")
                
                publish_results = await self.publisher.publish_with_quality_check(eap_doc)
                
                if publish_results.get("published"):
                    result.published = True
                    result.publish_platforms = publish_results.get("platforms", [])
                    result.publish_urls = [
                        r.get("url") for r in publish_results.get("results", [])
                        if r.get("url")
                    ]
            
            result.status = "completed"
            result.processing_time_ms = (time.time() - start_time) * 1000
            
            logger.info(f"âœ… Completed: {result.id} (quality: {result.quality_score:.2f}, gaps: {result.gaps_found})")
            
        except Exception as e:
            result.status = "failed"
            result.error = str(e)
            logger.error(f"âŒ Failed: {e}")
        
        self._results.append(result)
        return result
    
    async def process_item(self, item: SourceItem) -> FactoryResult:
        """Process a SourceItem from watcher"""
        return await self.process(
            content=item.content,
            title=item.title,
            topic=self._infer_topic(item),
            source_type=item.source_type,
            url=item.url,
            tags=item.tags
        )
    
    async def process_batch(self, items: List[SourceItem]) -> List[FactoryResult]:
        """Process multiple items"""
        results = []
        for item in items:
            result = await self.process_item(item)
            results.append(result)
            await asyncio.sleep(1)  # Rate limiting
        return results
    
    def _infer_topic(self, item: SourceItem) -> str:
        """Infer topic from source item"""
        type_to_topic = {
            SourceType.LAW: "legal governance",
            SourceType.POLICY: "policy analysis",
            SourceType.NEWS: "current affairs",
            SourceType.RESEARCH: "research analysis",
            SourceType.REPORT: "report analysis",
            SourceType.MANUAL: "general analysis"
        }
        return type_to_topic.get(item.source_type, "analysis")
    
    async def _create_basic_document(
        self,
        content: str,
        title: str,
        topic: str,
        tags: Optional[List[str]]
    ) -> Any:
        """Create basic EAP document without full EAP layer"""
        # This is a fallback when EAP layer is not available
        return None
    
    def _save_document(self, doc: Any) -> None:  # type: ignore[type-arg]
        """Save document to output directory"""
        try:
            # Save markdown
            md_path = f"{self.config.output_dir}/{doc.id}.md"
            with open(md_path, "w", encoding="utf-8") as f:
                f.write(doc.to_markdown())
            
            # Save JSON
            json_path = f"{self.config.output_dir}/{doc.id}.json"
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(doc.to_dict(), f, indent=2)
            
            logger.info(f"  â””â”€ Saved: {md_path}")
        except Exception as e:
            logger.error(f"Failed to save document: {e}")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # WATCHER MODE
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    def add_source(
        self,
        name: str,
        source_type: SourceType,
        url: Optional[str] = None,
        api_key: Optional[str] = None
    ):
        """Add source to watcher"""
        self.watcher.add_source(name, source_type, url, api_key)
    
    async def run_watcher_cycle(self) -> List[FactoryResult]:
        """Run one cycle of watcher mode"""
        logger.info("ğŸ” Running watcher cycle...")
        
        # Check sources
        items = await self.watcher.check_sources()
        
        if not items:
            logger.info("  â””â”€ No new items found")
            return []
        
        logger.info(f"  â””â”€ Found {len(items)} items")
        
        # Process items
        results = await self.process_batch(items)
        
        return results
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # CONTINUOUS MODE
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    async def run_continuous(self, interval_seconds: int = 3600):
        """Run in continuous mode"""
        logger.info(f"ğŸ”„ Starting continuous mode (interval: {interval_seconds}s)...")
        self._running = True
        
        while self._running:
            try:
                await self.run_watcher_cycle()
            except Exception as e:
                logger.error(f"Continuous cycle error: {e}")
            
            await asyncio.sleep(interval_seconds)
        
        logger.info("ğŸ›‘ Continuous mode stopped")
    
    def stop(self):
        """Stop continuous mode"""
        self._running = False
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # STATISTICS & REPORTING
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    def get_stats(self) -> Dict[str, Any]:
        """Get factory statistics"""
        return {
            "mode": self.config.mode.value,
            "documents_produced": len(self._documents_produced),
            "results_count": len(self._results),
            "completed": sum(1 for r in self._results if r.status == "completed"),
            "failed": sum(1 for r in self._results if r.status == "failed"),
            "published": sum(1 for r in self._results if r.published),
            "avg_quality": sum(r.quality_score for r in self._results) / max(1, len(self._results)),
            "total_gaps_found": sum(r.gaps_found for r in self._results),
            "watcher": self.watcher.get_stats(),
            "selector": self.selector.get_stats(),
            "ocean": self.ocean.get_stats(),
            "publisher": self.publisher.get_stats() if self.publisher else None
        }
    
    def get_recent_results(self, limit: int = 20) -> List[Dict]:
        """Get recent processing results"""
        return [r.to_dict() for r in self._results[-limit:]]
    
    def get_documents(self, limit: int = 10) -> List[Any]:  # type: ignore[type-arg]
        """Get recent produced documents"""
        return self._documents_produced[-limit:]
    
    def export_report(self, filepath: Optional[str] = None) -> str:
        """Export factory report"""
        report = {
            "generated_at": datetime.now(timezone.utc).isoformat(),
            "stats": self.get_stats(),
            "recent_results": self.get_recent_results(50)
        }
        
        filepath = filepath or f"{self.config.output_dir}/factory_report.json"
        
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2)
        
        logger.info(f"ğŸ“Š Exported report: {filepath}")
        return filepath


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONVENIENCE FUNCTIONS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Global instance
_factory_instance: Optional[ContentFactory] = None


def get_factory(config: Optional[FactoryConfig] = None) -> ContentFactory:
    """Get or create ContentFactory instance"""
    global _factory_instance
    if _factory_instance is None:
        _factory_instance = ContentFactory(config)
    return _factory_instance


async def quick_analyze(content: str, title: str = "Quick Analysis") -> Dict:
    """Quick analysis function"""
    factory = get_factory()
    result = await factory.process(content, title=title)
    return result.to_dict()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN (for testing)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    async def test_factory():
        print("=" * 80)
        print("ğŸ­ CLISONIX CONTENT FACTORY - Full Pipeline Test")
        print("=" * 80)
        
        # Create factory
        config = FactoryConfig(
            mode=FactoryMode.MANUAL,
            quality_threshold=0.6,  # Lower for testing
            auto_publish=False,      # Don't publish in test
            output_dir="output/content_factory_test"
        )
        
        factory = ContentFactory(config)
        
        # Test document
        test_content = """
        The European Commission has proposed a new AI Act amendment that would allow 
        private technology companies to operate cross-border AI identity verification 
        systems without mandatory transparency requirements. Under the proposed framework, 
        companies like Meta, Google, and Microsoft could authenticate citizens across 
        all 27 EU member states using proprietary algorithms.
        
        Critics argue this creates a dangerous precedent where sovereign identity 
        functions are effectively delegated to unaccountable private actors. The proposal 
        lacks clear mechanisms for citizen redress, has minimal auditing requirements, 
        and provides no framework for algorithmic explainability.
        
        Financial institutions and healthcare providers would be required to accept 
        these AI-driven identity verifications, creating systemic dependencies on 
        unregulated private infrastructure. Privacy advocates warn this could lead 
        to a permanent erosion of digital sovereignty within the European Union.
        """
        
        print("\nğŸ“„ Test Document:")
        print("-" * 40)
        print(test_content[:300] + "...")
        
        # Process through factory
        print("\n" + "=" * 80)
        print("ğŸ”„ PROCESSING THROUGH FULL PIPELINE")
        print("=" * 80)
        
        result = await factory.process(
            content=test_content,
            title="EU AI Act: Identity Sovereignty at Risk",
            topic="AI identity governance",
            tags=["eu", "ai-act", "identity", "sovereignty"]
        )
        
        print("\n" + "=" * 80)
        print("ğŸ“Š PROCESSING RESULT")
        print("=" * 80)
        
        print(f"\nResult ID: {result.id}")
        print(f"Status: {result.status}")
        print(f"Gaps Found: {result.gaps_found}")
        print(f"Discontinuity Level: {result.discontinuity_level}")
        print(f"Quality Score: {result.quality_score:.2f}")
        print(f"Published: {result.published}")
        print(f"Processing Time: {result.processing_time_ms:.1f}ms")
        
        if result.error:
            print(f"Error: {result.error}")
        
        # Get document
        print("\n" + "=" * 80)
        print("ğŸ“„ GENERATED DOCUMENT")
        print("=" * 80)
        
        docs = factory.get_documents(1)
        if docs:
            doc = docs[0]
            print(f"\nDocument ID: {doc.id}")
            print(f"Title: {doc.title}")
            print(f"Category: {doc.category}")
            print(f"Tags: {', '.join(doc.tags[:5])}")
            
            print("\n--- Executive Summary ---")
            print(doc.executive_summary[:400] + "...")
            
            print("\n--- Proposi (Paradigm) ---")
            print(f"Name: {doc.proposi.paradigm_name}")
            print(f"Description: {doc.proposi.paradigm_description[:200]}...")
        
        # Factory stats
        print("\n" + "=" * 80)
        print("ğŸ“Š FACTORY STATISTICS")
        print("=" * 80)
        
        stats = factory.get_stats()
        print(f"\nMode: {stats['mode']}")
        print(f"Documents Produced: {stats['documents_produced']}")
        print(f"Completed: {stats['completed']}")
        print(f"Failed: {stats['failed']}")
        print(f"Average Quality: {stats['avg_quality']:.2f}")
        print(f"Total Gaps Found: {stats['total_gaps_found']}")
        
        # Export report
        print("\n" + "=" * 80)
        print("ğŸ“Š EXPORTING REPORT")
        print("=" * 80)
        
        report_path = factory.export_report()
        print(f"Report saved: {report_path}")
        
        print("\n" + "=" * 80)
        print("âœ… CONTENT FACTORY TEST COMPLETE")
        print("=" * 80)
        
        print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       PIPELINE SUMMARY                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âœ… News/Law Watcher    â†’ Source ingestion ready                         â”‚
â”‚  âœ… BLERINA             â†’ Gap detection working                          â”‚
â”‚  âœ… EAP Layer           â†’ 3-stage processing complete                    â”‚
â”‚  âœ… Ocean Writer        â†’ Content enhancement active                     â”‚
â”‚  âœ… Quality Selector    â†’ Document filtering working                     â”‚
â”‚  âœ… CLX-Publisher       â†’ Multi-platform publishing ready                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """)
    
    asyncio.run(test_factory())
