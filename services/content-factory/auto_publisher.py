"""
CLISONIX AUTO-PUBLISHER - 100% Automated Content Pipeline
==========================================================

Fully automated system that:
1. Generates topics based on trends, news, and domain expertise
2. Creates high-quality documents via Blerina + EAP
3. Publishes to multiple platforms automatically
4. Runs 24/7 without human intervention

Target: 50-100 documents/day across all platforms
Goal: Make Clisonix known worldwide through valuable content

Author: Ledjan Ahmati (CEO, ABA GmbH)
"""

from __future__ import annotations

import asyncio
import hashlib
import json
import logging
import os
import random
from dataclasses import dataclass, field
from datetime import datetime, timezone
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional

# HTTP client
try:
    import httpx
except ImportError:
    httpx = None  # type: ignore

logger = logging.getLogger("auto_publisher")
logger.setLevel(logging.INFO)
if not logger.handlers:
    handler = logging.StreamHandler()
    handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
    logger.addHandler(handler)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# CONFIGURATION
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@dataclass
class AutoPublishConfig:
    """Configuration for auto-publishing"""
    # LLM
    ollama_url: str = field(default_factory=lambda: os.environ.get("OLLAMA_HOST", "http://localhost:11434"))
    model: str = field(default_factory=lambda: os.environ.get("MODEL", "llama3.1:8b"))

    # Publishing intervals
    min_interval_seconds: int = 300      # 5 minutes minimum between posts
    max_interval_seconds: int = 1800     # 30 minutes maximum
    docs_per_day_target: int = 50        # Target documents per day

    # Platforms
    linkedin_enabled: bool = True
    medium_enabled: bool = True
    twitter_enabled: bool = True
    devto_enabled: bool = True
    reddit_enabled: bool = True
    substack_enabled: bool = False       # Needs manual newsletter setup

    # API Credentials (from environment)
    linkedin_token: Optional[str] = field(default_factory=lambda: os.environ.get("LINKEDIN_ACCESS_TOKEN"))
    medium_token: Optional[str] = field(default_factory=lambda: os.environ.get("MEDIUM_TOKEN"))
    twitter_api_key: Optional[str] = field(default_factory=lambda: os.environ.get("TWITTER_API_KEY"))
    twitter_api_secret: Optional[str] = field(default_factory=lambda: os.environ.get("TWITTER_API_SECRET"))
    twitter_access_token: Optional[str] = field(default_factory=lambda: os.environ.get("TWITTER_ACCESS_TOKEN"))
    twitter_access_secret: Optional[str] = field(default_factory=lambda: os.environ.get("TWITTER_ACCESS_SECRET"))
    devto_api_key: Optional[str] = field(default_factory=lambda: os.environ.get("DEVTO_API_KEY"))
    reddit_client_id: Optional[str] = field(default_factory=lambda: os.environ.get("REDDIT_CLIENT_ID"))
    reddit_client_secret: Optional[str] = field(default_factory=lambda: os.environ.get("REDDIT_CLIENT_SECRET"))
    reddit_username: Optional[str] = field(default_factory=lambda: os.environ.get("REDDIT_USERNAME"))
    reddit_password: Optional[str] = field(default_factory=lambda: os.environ.get("REDDIT_PASSWORD"))
    reddit_subreddit: str = field(
        default_factory=lambda: os.environ.get(
            "REDDIT_SUBREDDIT", "clisonix"
        )
    )
    # Content storage
    output_dir: str = "/app/published"

    # Quality thresholds
    min_quality_score: float = 0.7
    max_gaps_allowed: int = 2


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# TOPIC DOMAINS - What Clisonix is about
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class TopicDomain(str, Enum):
    """Clisonix expertise domains"""
    EEG_NEUROSCIENCE = "eeg_neuroscience"
    AUDIO_PROCESSING = "audio_processing"
    AI_ML_SYSTEMS = "ai_ml_systems"
    HEALTHCARE_TECH = "healthcare_tech"
    SIGNAL_PROCESSING = "signal_processing"
    REGULATORY_COMPLIANCE = "regulatory_compliance"
    ENTERPRISE_AI = "enterprise_ai"
    EDGE_COMPUTING = "edge_computing"
    DATA_PRIVACY = "data_privacy"
    SUSTAINABLE_TECH = "sustainable_tech"


# Topic templates per domain
TOPIC_TEMPLATES: Dict[TopicDomain, List[str]] = {
    TopicDomain.EEG_NEUROSCIENCE: [
        "How real-time EEG analysis is revolutionizing {application}",
        "The science behind {technique} in brain-computer interfaces",
        "Why {metric} matters in clinical EEG monitoring",
        "Building reliable EEG systems for {use_case}",
        "Understanding {wave_type} waves and their clinical significance",
        "EEG-based {detection_type} detection: Current state and future",
        "Challenges in {challenge} for EEG devices",
        "How AI improves {aspect} in EEG interpretation",
    ],
    TopicDomain.AUDIO_PROCESSING: [
        "Real-time audio analysis for {application}",
        "Speech recognition in {environment}: Technical challenges",
        "How {technique} improves audio quality in {context}",
        "Building low-latency audio processing systems",
        "The role of {technology} in modern audio analytics",
        "Audio biomarkers: Detecting {condition} through voice",
    ],
    TopicDomain.AI_ML_SYSTEMS: [
        "Deploying ML models at scale: Lessons from {industry}",
        "Why {approach} is essential for production AI",
        "Building explainable AI systems for {domain}",
        "The architecture of real-time ML inference engines",
        "How {technique} improves model performance in {context}",
        "Edge AI vs Cloud AI: When to use what",
        "Monitoring ML models in production: Best practices",
    ],
    TopicDomain.HEALTHCARE_TECH: [
        "Digital health compliance: Navigating {regulation}",
        "Building HIPAA-compliant AI systems",
        "The future of remote patient monitoring",
        "AI in diagnostics: {modality} analysis systems",
        "Interoperability challenges in healthcare IT",
        "Patient data privacy in the age of AI",
    ],
    TopicDomain.SIGNAL_PROCESSING: [
        "Real-time signal filtering for {application}",
        "Understanding {algorithm} in digital signal processing",
        "Time-frequency analysis: {method} explained",
        "Artifact removal in biomedical signals",
        "Building robust signal processing pipelines",
    ],
    TopicDomain.REGULATORY_COMPLIANCE: [
        "EU AI Act: What it means for {industry}",
        "MDR compliance for AI-powered medical devices",
        "GDPR and AI: Handling personal data correctly",
        "ISO 13485 for software as medical device",
        "Building audit trails for regulatory compliance",
    ],
    TopicDomain.ENTERPRISE_AI: [
        "Enterprise AI adoption: Common pitfalls and solutions",
        "Building AI centers of excellence",
        "ROI of AI investments: Measuring real value",
        "AI governance in large organizations",
        "From POC to production: Scaling enterprise AI",
    ],
    TopicDomain.EDGE_COMPUTING: [
        "Edge AI for {application}: Architecture patterns",
        "Optimizing neural networks for edge deployment",
        "Real-time inference on embedded systems",
        "Power-efficient AI at the edge",
        "Edge-cloud hybrid architectures for AI",
    ],
    TopicDomain.DATA_PRIVACY: [
        "Federated learning: AI without data sharing",
        "Differential privacy in ML systems",
        "Anonymization techniques for health data",
        "Privacy-preserving AI: Technical approaches",
        "GDPR-compliant machine learning pipelines",
    ],
    TopicDomain.SUSTAINABLE_TECH: [
        "Green AI: Reducing the carbon footprint of ML",
        "Energy-efficient model architectures",
        "Sustainable data centers for AI workloads",
        "The environmental cost of large language models",
        "Building eco-friendly AI infrastructure",
    ],
}

# Fill-in values for templates
TEMPLATE_FILLS: Dict[str, List[str]] = {
    "application": [
        "sleep monitoring", "epilepsy detection",
        "attention tracking", "meditation apps",
        "mental health assessment",
        "cognitive load measurement",
        "driver alertness", "anesthesia monitoring"
    ],
    "technique": [
        "spectral analysis", "wavelet decomposition",
        "independent component analysis",
        "deep learning", "adaptive filtering",
        "coherence analysis"
    ],
    "metric": ["signal quality", "artifact rejection rate", "classification accuracy", "latency", "battery efficiency"],
    "use_case": ["clinical research", "consumer wearables", "hospital ICU", "home monitoring", "sports performance"],
    "wave_type": ["alpha", "beta", "theta", "delta", "gamma"],
    "detection_type": ["seizure", "sleep stage", "drowsiness", "stress", "cognitive impairment"],
    "challenge": [
        "motion artifact removal",
        "electrode contact quality",
        "signal standardization",
        "real-time processing"
    ],
    "aspect": ["accuracy", "speed", "reliability", "interpretability"],
    "environment": ["noisy environments", "clinical settings", "call centers", "smart homes"],
    "context": ["telehealth", "voice assistants", "medical diagnostics", "security systems"],
    "technology": ["transformer models", "spectrogram analysis", "voice activity detection", "speaker diarization"],
    "condition": ["Parkinson's disease", "depression", "respiratory illness", "stress levels"],
    "industry": ["healthcare", "finance", "manufacturing", "retail"],
    "approach": ["MLOps", "model versioning", "A/B testing", "continuous training"],
    "domain": ["healthcare", "finance", "legal", "insurance"],
    "regulation": ["EU AI Act", "FDA guidelines", "HIPAA", "MDR"],
    "modality": ["radiology", "pathology", "cardiology", "dermatology"],
    "algorithm": ["FFT", "Kalman filter", "IIR filters", "FIR filters"],
    "method": ["STFT", "wavelet transform", "Hilbert-Huang transform", "spectrogram"],
}


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# TOPIC GENERATOR
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class TopicGenerator:
    """Generates topics for content creation"""

    def __init__(self, config: AutoPublishConfig):
        self.config = config
        self.generated_topics: List[str] = []
        self.topic_hashes: set = set()  # Avoid duplicates

    def generate_from_template(self) -> Dict[str, Any]:
        """Generate a topic from templates"""
        # Pick random domain
        domain = random.choice(list(TopicDomain))
        templates = TOPIC_TEMPLATES.get(domain, [])

        if not templates:
            return self._fallback_topic(domain)

        # Pick random template
        template = random.choice(templates)

        # Fill in placeholders
        topic = template
        for placeholder, values in TEMPLATE_FILLS.items():
            if "{" + placeholder + "}" in topic:
                topic = topic.replace("{" + placeholder + "}", random.choice(values))

        # Check for duplicate
        topic_hash = hashlib.md5(topic.lower().encode()).hexdigest()
        if topic_hash in self.topic_hashes:
            return self.generate_from_template()  # Try again

        self.topic_hashes.add(topic_hash)
        self.generated_topics.append(topic)

        return {
            "topic": topic,
            "domain": domain.value,
            "generated_at": datetime.now(timezone.utc).isoformat(),
            "method": "template"
        }

    async def generate_with_llm(self, context: Optional[str] = None) -> Dict[str, Any]:
        """Generate topic using LLM for more variety"""
        domain = random.choice(list(TopicDomain))

        prompt = f"""Generate a unique, engaging article topic for a technology company specializing in:
- EEG/brain signal processing
- Audio analytics
- AI/ML systems for healthcare
- Medical device software

Domain focus: {domain.value.replace('_', ' ').title()}

{f'Context: {context}' if context else ''}

Requirements:
- Professional but accessible tone
- Practical and actionable
- Appeal to CTOs, developers, and healthcare professionals
- 10-15 words maximum

Return ONLY the topic title, nothing else."""

        try:
            if httpx:
                async with httpx.AsyncClient(timeout=30.0) as client:
                    response = await client.post(
                        f"{self.config.ollama_url}/api/generate",
                        json={
                            "model": self.config.model,
                            "prompt": prompt,
                            "stream": False
                        }
                    )

                    if response.status_code == 200:
                        result = response.json()
                        topic = result.get("response", "").strip().strip('"\'')

                        if topic and len(topic) > 10:
                            topic_hash = hashlib.md5(topic.lower().encode()).hexdigest()
                            if topic_hash not in self.topic_hashes:
                                self.topic_hashes.add(topic_hash)
                                self.generated_topics.append(topic)

                                return {
                                    "topic": topic,
                                    "domain": domain.value,
                                    "generated_at": datetime.now(timezone.utc).isoformat(),
                                    "method": "llm"
                                }
        except Exception as e:
            logger.warning("LLM topic generation failed: %s", e)

        # Fallback to template
        return self.generate_from_template()

    def _fallback_topic(self, domain: TopicDomain) -> Dict[str, Any]:
        """Fallback topic generation"""
        topics = [
            "Building Reliable AI Systems for Healthcare Applications",
            "Real-Time Signal Processing: Architecture and Best Practices",
            "The Future of Brain-Computer Interfaces in Consumer Tech",
            "AI Compliance in Medical Devices: A Practical Guide",
            "Edge Computing for Healthcare: When Milliseconds Matter",
        ]
        topic = random.choice(topics)
        return {
            "topic": topic,
            "domain": domain.value,
            "generated_at": datetime.now(timezone.utc).isoformat(),
            "method": "fallback"
        }


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# CLISONIX IDENTITY & TECHNOLOGY
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

CLISONIX_TECH_IDENTITY = {
    "tide_engine": {
        "name": "Tide Engine",
        "description": "Real-time data synchronization using tidal flow patterns",
        "use_case": "Ensures consistent state across distributed healthcare nodes"
    },
    "signal_fabric": {
        "name": "Signal Fabric",
        "description": "Neural mesh for interconnected signal processing",
        "use_case": "Weaves together EEG, audio, and biosensor streams"
    },
    "neural_mesh": {
        "name": "Neural Mesh",
        "description": "Distributed neural network topology",
        "use_case": "Enables edge-to-cloud AI inference with sub-ms latency"
    },
    "crdt_merge": {
        "name": "CRDT Merge Layer",
        "description": "Conflict-free replicated data types for collaboration",
        "use_case": "Multiple clinicians editing same patient data simultaneously"
    },
    "pq_encryption": {
        "name": "Post-Quantum Encryption",
        "description": "Kyber-1024 lattice-based cryptography",
        "use_case": "Future-proof security for sensitive medical data"
    },
    "liam_algebra": {
        "name": "LIAM Binary Algebra",
        "description": "Labor Intelligence Array Matrix - no-loop vectorized processing",
        "use_case": "High-performance signal transformations without Python loops"
    },
    "alda_orchestration": {
        "name": "ALDA Labor Array",
        "description": "Artificial Labor Determined Array for workload distribution",
        "use_case": "Deterministic task scheduling across compute nodes"
    }
}


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# CONTENT GENERATOR - Premium Clisonix Articles
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class ContentGenerator:
    """
    Generates premium Clisonix articles with:
    - Real data tables from LIAM/ALDA/Excel Core
    - Clisonix technology identity
    - Technical depth with code snippets
    - Professional but distinctive voice
    """

    def __init__(self, config: AutoPublishConfig):
        self.config = config
        self.intelligence_lab_url = os.environ.get(
            "INTELLIGENCE_LAB_URL",
            "http://clisonix-intelligence-lab:8099"
        )

    async def _fetch_tables(self) -> Dict[str, Any]:
        """Fetch real data tables from Intelligence Lab"""
        try:
            if httpx:
                async with httpx.AsyncClient(timeout=10.0) as client:
                    resp = await client.get(f"{self.intelligence_lab_url}/tables/all")
                    if resp.status_code == 200:
                        return resp.json()
        except Exception as e:
            logger.warning("Could not fetch tables: %s", e)
        return {}

    def _select_clisonix_tech(self, domain: str) -> List[Dict[str, Any]]:
        """Select relevant Clisonix technologies for the domain"""
        tech_map = {
            "eeg_neuroscience": ["signal_fabric", "neural_mesh", "liam_algebra"],
            "audio_processing": ["signal_fabric", "tide_engine"],
            "ai_ml_systems": ["neural_mesh", "liam_algebra", "alda_orchestration"],
            "healthcare_tech": ["crdt_merge", "pq_encryption", "tide_engine"],
            "regulatory": ["pq_encryption", "crdt_merge"],
            "enterprise_ai": ["alda_orchestration", "tide_engine", "neural_mesh"],
            "real_time_systems": ["tide_engine", "signal_fabric", "neural_mesh"]
        }

        tech_keys = tech_map.get(domain, ["tide_engine", "signal_fabric"])
        return [CLISONIX_TECH_IDENTITY[k] for k in tech_keys if k in CLISONIX_TECH_IDENTITY]

    def _generate_code_snippet(self, domain: str) -> str:
        """Generate relevant code snippet for the domain"""
        snippets = {
            "eeg_neuroscience": '''```python
# Clisonix Signal Fabric - EEG Processing
from clisonix.signal import SignalFabric

fabric = SignalFabric(channels=64, sample_rate=256)
filtered = fabric.bandpass_filter(raw_eeg, low=0.5, high=45)
features = fabric.extract_features(filtered, bands=['alpha', 'beta', 'gamma'])
```''',
            "ai_ml_systems": '''```python
# LIAM Binary Algebra - Vectorized Processing
from clisonix.liam import BinaryAlgebra

algebra = BinaryAlgebra()
transformed = algebra.transform_matrix(data, weights)
compressed = algebra.svd_compress(transformed, k=32)
```''',
            "real_time_systems": '''```python
# Tide Engine - Real-time Sync
from clisonix.tide import TideEngine

tide = TideEngine(nodes=['edge-1', 'edge-2', 'cloud'])
tide.sync_state(patient_data, consistency='eventual')
```''',
            "healthcare_tech": '''```python
# CRDT Merge - Collaborative Editing
from clisonix.crdt import DocumentMerge

doc = DocumentMerge(document_id='patient-123')
doc.apply_edit(clinician_a_changes)
doc.apply_edit(clinician_b_changes)  # No conflicts!
merged = doc.get_state()
```''',
            "enterprise_ai": '''```python
# ALDA Labor Orchestration
from clisonix.alda import LaborArray

array = LaborArray(dimension=64, determinism='strict')
array.schedule_task(inference_job, priority=1)
results = array.execute_all()
```'''
        }
        return snippets.get(domain, snippets["ai_ml_systems"])

    def _get_unsplash_images(self, domain: str, topic: str) -> List[Dict[str, str]]:
        """Get relevant Unsplash image URLs based on domain/topic"""
        # Curated Unsplash images by domain (free, no API key needed)
        image_map = {
            "eeg_neuroscience": [
                {
                    "url": "https://images.unsplash.com/photo-1559757175-5700dde675bc?w=800&q=80",
                    "alt": "Brain neural network visualization",
                    "credit": "Unsplash"
                },
                {
                    "url": "https://images.unsplash.com/photo-1507003211169-0a1dd7228f2d?w=800&q=80",
                    "alt": "Neuroscience research lab",
                    "credit": "Unsplash"
                },
                {
                    "url": "https://images.unsplash.com/photo-1530026405186-ed1f139313f8?w=800&q=80",
                    "alt": "Brain scan imaging",
                    "credit": "Unsplash"
                },
            ],
            "audio_processing": [
                {
                    "url": "https://images.unsplash.com/photo-1558618666-fcd25c85f82e?w=800&q=80",
                    "alt": "Sound wave visualization",
                    "credit": "Unsplash"
                },
                {
                    "url": "https://images.unsplash.com/photo-1511379938547-c1f69419868d?w=800&q=80",
                    "alt": "Audio processing equipment",
                    "credit": "Unsplash"
                },
                {
                    "url": "https://images.unsplash.com/photo-1598488035139-bdbb2231ce04?w=800&q=80",
                    "alt": "Digital audio waveform",
                    "credit": "Unsplash"
                },
            ],
            "ai_ml_systems": [
                {
                    "url": "https://images.unsplash.com/photo-1677442136019-21780ecad995?w=800&q=80",
                    "alt": "AI artificial intelligence concept",
                    "credit": "Unsplash"
                },
                {
                    "url": "https://images.unsplash.com/photo-1555255707-c07966088b7b?w=800&q=80",
                    "alt": "Machine learning network",
                    "credit": "Unsplash"
                },
                {
                    "url": "https://images.unsplash.com/photo-1620712943543-bcc4688e7485?w=800&q=80",
                    "alt": "AI robot technology",
                    "credit": "Unsplash"
                },
            ],
            "healthcare_tech": [
                {
                    "url": "https://images.unsplash.com/photo-1576091160399-112ba8d25d1d?w=800&q=80",
                    "alt": "Healthcare technology",
                    "credit": "Unsplash"
                },
                {
                    "url": "https://images.unsplash.com/photo-1551076805-e1869033e561?w=800&q=80",
                    "alt": "Medical innovation",
                    "credit": "Unsplash"
                },
                {
                    "url": "https://images.unsplash.com/photo-1530497610245-b1acf56b0838?w=800&q=80",
                    "alt": "Digital health monitoring",
                    "credit": "Unsplash"
                },
            ],
            "regulatory": [
                {
                    "url": "https://images.unsplash.com/photo-1589829545856-d10d557cf95f?w=800&q=80",
                    "alt": "Compliance and regulation",
                    "credit": "Unsplash"
                },
                {
                    "url": "https://images.unsplash.com/photo-1450101499163-c8848c66ca85?w=800&q=80",
                    "alt": "Legal documentation",
                    "credit": "Unsplash"
                },
                {
                    "url": "https://images.unsplash.com/photo-1507925921958-8a62f3d1a50d?w=800&q=80",
                    "alt": "Data security shield",
                    "credit": "Unsplash"
                },
            ],
            "enterprise_ai": [
                {
                    "url": "https://images.unsplash.com/photo-1451187580459-43490279c0fa?w=800&q=80",
                    "alt": "Enterprise technology globe",
                    "credit": "Unsplash"
                },
                {
                    "url": "https://images.unsplash.com/photo-1518770660439-4636190af475?w=800&q=80",
                    "alt": "Circuit board technology",
                    "credit": "Unsplash"
                },
                {
                    "url": "https://images.unsplash.com/photo-1504868584819-f8e8b4b6d7e3?w=800&q=80",
                    "alt": "Data analytics dashboard",
                    "credit": "Unsplash"
                },
            ],
            "real_time_systems": [
                {
                    "url": "https://images.unsplash.com/photo-1551288049-bebda4e38f71?w=800&q=80",
                    "alt": "Real-time data monitoring",
                    "credit": "Unsplash"
                },
                {
                    "url": "https://images.unsplash.com/photo-1558494949-ef010cbdcc31?w=800&q=80",
                    "alt": "Server infrastructure",
                    "credit": "Unsplash"
                },
                {
                    "url": "https://images.unsplash.com/photo-1460925895917-afdab827c52f?w=800&q=80",
                    "alt": "Live analytics screen",
                    "credit": "Unsplash"
                },
            ],
        }
        return image_map.get(domain, image_map["ai_ml_systems"])

    def _get_youtube_embed(self, domain: str) -> Dict[str, str]:
        """Get relevant Clisonix/tech YouTube video embed by domain"""
        video_map = {
            "eeg_neuroscience": {"id": "rSQfAR_Kn9k", "title": "How EEG Brain-Computer Interfaces Work"},
            "audio_processing": {"id": "tDnMOLgWmQE", "title": "Digital Signal Processing Explained"},
            "ai_ml_systems": {"id": "aircAruvnKk", "title": "How AI Is Transforming Healthcare"},
            "healthcare_tech": {"id": "aircAruvnKk", "title": "AI in Healthcare - The Future"},
            "regulatory": {"id": "Aw16LPUsEk0", "title": "Healthcare Data Compliance Explained"},
            "enterprise_ai": {"id": "ad79nYk2keg", "title": "Enterprise AI Architecture"},
            "real_time_systems": {"id": "LjSM8yy3FEI", "title": "Real-Time Data Processing Systems"},
        }
        return video_map.get(domain, video_map["ai_ml_systems"])

    def _inject_images_into_content(self, content: str, images: List[Dict[str, str]]) -> str:
        """Replace [IMAGE: ...] markers with actual image markdown"""
        import re
        img_index = 0

        def replace_marker(match):
            nonlocal img_index
            if img_index < len(images):
                img = images[img_index]
                img_index += 1
                return f'\n\n![{img["alt"]}]({img["url"]})\n*{img["alt"]} ‚Äî Photo: {img["credit"]}*\n\n'
            return ""
        return re.sub(r'\[IMAGE:\s*[^\]]*\]', replace_marker, content)

    async def generate_article(self, topic: str, domain: str) -> Dict[str, Any]:
        """Generate premium article with Clisonix identity, images, video, and FAQ"""

        # Fetch real tables
        tables = await self._fetch_tables()

        # Select relevant Clisonix tech
        tech_stack = self._select_clisonix_tech(domain)
        tech_names = [t["name"] for t in tech_stack]

        # Get code snippet
        code_snippet = self._generate_code_snippet(domain)

        # Build enhanced prompt
        default_table = "| Metric | Value |\n|--------|-------|\n| Example | 42 |"
        metrics_table = tables.get('system_metrics', {}).get('markdown', default_table)
        prompt = f"""Write a premium technical article for Clisonix, a healthcare AI company.

TOPIC: "{topic}"
DOMAIN: {domain.replace('_', ' ').title()}

CLISONIX IDENTITY (weave these naturally into the article):
{json.dumps([{"name": t["name"], "use": t["use_case"]} for t in tech_stack], indent=2)}

STRUCTURE:
1. **Hook** - Why this matters NOW (not generic intro)
2. **The Problem** - Real challenges in {domain.replace('_', ' ')}
3. **Technical Deep Dive** - Architecture, algorithms, implementation
4. **Real Data** - Include this table in the article:
{metrics_table}

5. **Code Example** - Include this snippet:
{code_snippet}

6. **Results & Impact** - Measurable outcomes
7. **What's Next** - Future directions with clear CTA
8. **FAQ** - Add a "Frequently Asked Questions" section with 4-5 Q&A pairs relevant to the topic. Format each as:
   **Q: Question here?**
   A: Answer here.

IMAGE PLACEMENT:
- After section 1 (Hook), write: [IMAGE: description of a relevant hero image for {domain.replace('_', ' ')}]
- After section 3 (Technical Deep Dive), write: [IMAGE: description of architecture/diagram image]
- After section 6 (Results), write: [IMAGE: description of results/chart image]

REQUIREMENTS:
- 1200-1800 words (including FAQ)
- Technical but accessible
- Include the Clisonix technologies listed above naturally
- Use the provided table and code snippet
- Include the FAQ section with real, useful answers
- Include [IMAGE: ...] markers where images should go
- End with specific CTA (GitHub, demo, contact)
- Voice: Expert lab, not marketing

Write the complete article:"""

        try:
            if httpx:
                async with httpx.AsyncClient(timeout=120.0) as client:
                    response = await client.post(
                        f"{self.config.ollama_url}/api/generate",
                        json={
                            "model": self.config.model,
                            "prompt": prompt,
                            "stream": False
                        }
                    )

                    if response.status_code == 200:
                        result = response.json()
                        content = result.get("response", "").strip()

                        if content and len(content) > 500:
                            # Enrich content with real images and video
                            images = self._get_unsplash_images(domain, topic)
                            video = self._get_youtube_embed(domain)
                            content = self._inject_images_into_content(content, images)

                            return {
                                "success": True,
                                "title": topic,
                                "content": content,
                                "word_count": len(content.split()),
                                "domain": domain,
                                "clisonix_tech": tech_names,
                                "has_table": bool(tables),
                                "has_code": True,
                                "has_faq": "FAQ" in content or "Frequently Asked" in content,
                                "has_images": True,
                                "images": images,
                                "video": video,
                                "seo_description": content[:155].replace('\n', ' ').strip(),
                                "generated_at": datetime.now(timezone.utc).isoformat()
                            }
        except Exception as e:
            logger.error("Content generation failed: %s", e)

        return {
            "success": False,
            "title": topic,
            "error": "Content generation failed",
            "domain": domain
        }

    def generate_social_variants(self, article: Dict[str, Any]) -> Dict[str, str]:
        """Generate platform-specific variants"""
        title = article.get("title", "")
        content = article.get("content", "")

        # Extract first paragraph as summary
        paragraphs = [p.strip() for p in content.split("\n\n") if p.strip() and not p.startswith("#")]
        summary = paragraphs[0] if paragraphs else content[:300]

        return {
            "linkedin": f"üöÄ {title}\n\n{summary[:500]}...\n\n#HealthTech #AI #Innovation #Clisonix",
            "twitter": f"{title[:200]}\n\n{summary[:180]}...\n\n#HealthTech #AI",
            "reddit_title": title,
            "reddit_body": (
                f"{summary[:800]}\n\n---\n"
                "*Published by [Clisonix]"
                "(https://clisonix.com)"
                " - AI-Powered Healthcare"
                " Intelligence*"
            ),
            "medium_excerpt": summary[:400],
            "devto_tags": "healthtech,ai,machinelearning,programming"
        }


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# PLATFORM PUBLISHERS
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class LocalPublisher:
    """Save articles locally - always works, no API needed"""

    def __init__(self, output_dir: str = "/app/published"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        # Also create blog directory for Jekyll/GitHub Pages
        self.blog_dir = self.output_dir / "blog" / "_posts"
        self.blog_dir.mkdir(parents=True, exist_ok=True)

    async def publish(
        self,
        content: str,
        title: str,
        domain: str = "general",
        tags: Optional[List[str]] = None,
        clisonix_tech: Optional[List[str]] = None,
        has_table: bool = False,
        has_code: bool = False,
        video: Optional[Dict[str, str]] = None,
        images: Optional[List[Dict[str, str]]] = None
    ) -> Dict[str, Any]:
        """Save article locally as Markdown and HTML with enhanced metadata"""
        try:
            # Create slug from title
            slug = title.lower()
            for char in ["'", '"', ":", ";", ",", ".", "?", "!", "(", ")", "[", "]"]:
                slug = slug.replace(char, "")
            slug = slug.replace(" ", "-")[:50]

            date_str = datetime.now(timezone.utc).strftime("%Y-%m-%d")
            timestamp = datetime.now(timezone.utc).isoformat()

            # Jekyll-compatible filename
            filename = f"{date_str}-{slug}"

            actual_tags = tags if tags else ["clisonix", "technology", "ai"]
            tech_stack = clisonix_tech if clisonix_tech else []

            # Get first image for og:image if available
            og_image = "https://clisonix.com/images/clisonix-og-default.png"
            if clisonix_tech:  # reuse param to pass images through pipeline
                pass  # og_image set below from content

            # Extract first image URL from content for og:image
            import re as _re
            img_match = _re.search(r'!\[.*?\]\((https://[^)]+)\)', content)
            if img_match:
                og_image = img_match.group(1)

            seo_desc = content[:155].replace('\n', ' ').replace('"', "'").strip()
            canonical_slug = slug

            # Create enhanced Jekyll front matter with SEO + technical metadata
            front_matter = f"""---
layout: post
title: "{title}"
date: {timestamp}
categories: [{domain}]
tags: [{', '.join(actual_tags)}]
author: Clisonix AI
description: "{seo_desc}"
image: "{og_image}"
canonical_url: "https://ledjanahmati.github.io/clisonix-blog/static/{date_str}-{canonical_slug}.html"
clisonix_tech: [{', '.join(tech_stack)}]
has_table: {str(has_table).lower()}
has_code: {str(has_code).lower()}
has_faq: true
lab_generated: true
---

"""
            # Save as Markdown (Jekyll format)
            md_path = self.blog_dir / f"{filename}.md"
            md_path.write_text(front_matter + content, encoding="utf-8")

            # Save as plain Markdown (without front matter)
            plain_md_path = self.output_dir / "articles" / f"{filename}.md"
            plain_md_path.parent.mkdir(parents=True, exist_ok=True)
            plain_md_path.write_text(f"# {title}\n\n{content}", encoding="utf-8")

            # Create HTML version
            html_content = self._markdown_to_html(
                title, content, actual_tags, domain,
                video=video, images=images
            )
            html_path = self.output_dir / "html" / f"{filename}.html"
            html_path.parent.mkdir(parents=True, exist_ok=True)
            html_path.write_text(html_content, encoding="utf-8")

            # Update index
            self._update_index(title, filename, domain, actual_tags)

            logger.info("üìÅ Saved locally: %s", filename)

            return {
                "success": True,
                "platform": "local",
                "files": {
                    "jekyll": str(md_path),
                    "markdown": str(plain_md_path),
                    "html": str(html_path)
                },
                "slug": slug,
                "url": f"/blog/{date_str}/{slug}",
                "published_at": timestamp
            }

        except Exception as e:
            logger.error("Local save failed: %s", e)
            return {"success": False, "error": str(e), "platform": "local"}

    @staticmethod
    def _table_to_html(markdown_table: str) -> str:
        """Convert markdown table to styled HTML table"""
        lines = [line.strip() for line in markdown_table.strip().split('\n') if line.strip()]
        if len(lines) < 2:
            return markdown_table
        headers = [c.strip() for c in lines[0].split('|') if c.strip()]
        html = '<table><thead><tr>' + ''.join(f'<th>{h}</th>' for h in headers) + '</tr></thead><tbody>'
        for row_line in lines[2:]:
            cells = [c.strip() for c in row_line.split('|') if c.strip()]
            html += '<tr>' + ''.join(f'<td>{c}</td>' for c in cells) + '</tr>'
        html += '</tbody></table>'
        return html

    def _markdown_to_html(self, title: str, content: str, tags: List[str], domain: str,
                          video: Optional[Dict[str, str]] = None,
                          images: Optional[List[Dict[str, str]]] = None) -> str:
        """Convert markdown to rich HTML with SEO, images, video, FAQ, and Schema.org"""
        import re
        html_body = content

        # Convert headers
        for i in range(6, 0, -1):
            pattern = re.compile(r'^' + '#' * i + r' (.+)$', re.MULTILINE)
            html_body = pattern.sub(rf'<h{i}>\1</h{i}>', html_body)

        # Convert images: ![alt](url) ‚Üí <figure>
        html_body = re.sub(
            r'!\[([^\]]*)\]\(([^)]+)\)',
            r'<figure><img src="\2" alt="\1"'
            r' loading="lazy"'
            r' style="width:100%;border-radius:12px;'
            r'margin:1.5rem 0">'
            r'<figcaption>\1</figcaption></figure>',
            html_body
        )

        # Convert bold and italic
        html_body = re.sub(r'\*\*(.+?)\*\*', r'<strong>\1</strong>', html_body)
        html_body = re.sub(r'\*(.+?)\*', r'<em>\1</em>', html_body)

        # Convert code blocks
        html_body = re.sub(
            r'```(\w+)?\n(.*?)```',
            r'<pre><code class="language-\1">\2</code></pre>',
            html_body, flags=re.DOTALL
        )
        html_body = re.sub(r'`([^`]+)`', r'<code>\1</code>', html_body)

        # Convert tables
        html_body = re.sub(
            r'\|([^\n]+)\|\n\|[-| ]+\|\n((?:\|[^\n]+\|\n?)+)',
            lambda m: self._table_to_html(m.group(0)),
            html_body
        )

        # Convert paragraphs
        paragraphs = html_body.split('\n\n')
        html_body = '\n'.join([
            f'<p>{p.strip()}</p>' if p.strip() and not p.strip().startswith('<') else p
            for p in paragraphs if p.strip()
        ])

        # Style FAQ section
        html_body = re.sub(
            r'<strong>Q:\s*(.+?)\?</strong>',
            r'<div class="faq-item"><h4 class="faq-q">‚ùì \1?</h4>',
            html_body
        )
        html_body = re.sub(
            r'A:\s*(.+?)(?=<div class="faq-item">|</div>|$)',
            r'<p class="faq-a">\1</p></div>',
            html_body, flags=re.DOTALL
        )

        # Build video embed HTML
        video_html = ""
        if video:
            video_html = f'''
    <section class="video-section">
        <h3>üé• {video.get("title", "Watch Related Video")}</h3>
        <div class="video-container">
            <iframe src="https://www.youtube.com/embed/{video["id"]}"
                    title="{video.get("title", "Video")}"
                    frameborder="0"
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                    allowfullscreen loading="lazy"></iframe>
        </div>
    </section>'''

        # og:image
        og_image = "https://clisonix.com/images/clisonix-og-default.png"
        if images and len(images) > 0:
            og_image = images[0].get("url", og_image)

        seo_desc = title[:155]
        date_str = datetime.now(timezone.utc).strftime('%Y-%m-%d')
        date_display = datetime.now(timezone.utc).strftime('%B %d, %Y')
        canonical = f"https://ledjanahmati.github.io/clisonix-blog/static/{date_str}-{domain}.html"
        keywords = ', '.join(tags + ["clisonix", "healthcare AI", "industrial intelligence"])

        # Schema.org Article structured data
        schema_data = {
            "@context": "https://schema.org",
            "@type": "TechArticle",
            "headline": title,
            "description": seo_desc,
            "image": og_image,
            "author": {"@type": "Organization", "name": "Clisonix", "url": "https://clisonix.com"},
            "publisher": {"@type": "Organization", "name": "Clisonix - ABA GmbH", "url": "https://clisonix.com"},
            "datePublished": date_str,
            "dateModified": date_str,
            "mainEntityOfPage": canonical,
            "keywords": keywords
        }
        schema_json = json.dumps(schema_data, ensure_ascii=False)

        # FAQ Schema (if FAQ section exists)
        faq_schema = ""
        faq_matches = re.findall(r'‚ùì\s*(.+?\?).+?class="faq-a">(.+?)</p>', html_body, re.DOTALL)
        if faq_matches:
            faq_entries = [
                {"@type": "Question", "name": q.strip(), "acceptedAnswer": {"@type": "Answer", "text": a.strip()}}
                for q, a in faq_matches[:10]
            ]
            faq_data = {
                "@context": "https://schema.org",
                "@type": "FAQPage",
                "mainEntity": faq_entries
            }
            faq_schema = f'''
    <script type="application/ld+json">
    {json.dumps(faq_data, ensure_ascii=False)}
    </script>'''

        return f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="{seo_desc}">
    <meta name="keywords" content="{keywords}">
    <meta name="author" content="Clisonix">
    <meta name="robots" content="index, follow">
    <link rel="canonical" href="{canonical}">
    <!-- Open Graph -->
    <meta property="og:type" content="article">
    <meta property="og:title" content="{title}">
    <meta property="og:description" content="{seo_desc}">
    <meta property="og:image" content="{og_image}">
    <meta property="og:url" content="{canonical}">
    <meta property="og:site_name" content="Clisonix Blog">
    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="{title}">
    <meta name="twitter:description" content="{seo_desc}">
    <meta name="twitter:image" content="{og_image}">
    <title>{title} | Clisonix Blog</title>
    <!-- Schema.org -->
    <script type="application/ld+json">{schema_json}</script>
    {faq_schema}
    <style>
        :root {{ --primary: #2563eb; --accent: #00d4ff; --bg: #f8fafc; --text: #1e293b; --card: #fff; }}
        * {{ margin: 0; padding: 0; box-sizing: border-box; }}
        body {{ font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
               max-width: 860px; margin: 0 auto; padding: 2rem 1.5rem; background: var(--bg); color: var(--text); line-height: 1.8; }}
        h1 {{ color: var(--primary); font-size: 2.2rem; border-bottom: 3px solid var(--accent); padding-bottom: 0.75rem; margin-bottom: 0.5rem; }}
        h2 {{ color: var(--primary); margin: 2rem 0 1rem; font-size: 1.5rem; }}
        h3 {{ color: #334155; margin: 1.5rem 0 0.75rem; }}
        .meta {{ color: #64748b; font-size: 0.9rem; margin-bottom: 1.5rem; display: flex; gap: 1rem; flex-wrap: wrap; }}
        .tags {{ display: flex; gap: 0.5rem; flex-wrap: wrap; margin-bottom: 2rem; }}
        .tag {{ background: var(--primary); color: white; padding: 0.25rem 0.75rem; border-radius: 1rem; font-size: 0.8rem; text-decoration: none; }}
        .content {{ margin-bottom: 2rem; }}
        .content p {{ margin-bottom: 1rem; }}
        .content img {{ max-width: 100%; height: auto; border-radius: 12px; margin: 1.5rem 0; box-shadow: 0 4px 20px rgba(0,0,0,0.1); }}
        figure {{ margin: 2rem 0; text-align: center; }}
        figcaption {{ color: #64748b; font-size: 0.85rem; margin-top: 0.5rem; font-style: italic; }}
        pre {{ background: #1e293b; color: #e2e8f0; padding: 1.5rem; border-radius: 12px; overflow-x: auto; margin: 1.5rem 0; }}
        code {{ font-family: 'Fira Code', 'Consolas', monospace; font-size: 0.9rem; }}
        p code {{ background: #e2e8f0; padding: 0.15rem 0.4rem; border-radius: 4px; color: var(--primary); }}
        table {{ width: 100%; border-collapse: collapse; margin: 1.5rem 0; background: var(--card); border-radius: 8px; overflow: hidden; box-shadow: 0 2px 8px rgba(0,0,0,0.06); }}
        th {{ background: var(--primary); color: white; padding: 0.75rem 1rem; text-align: left; }}
        td {{ padding: 0.75rem 1rem; border-bottom: 1px solid #e2e8f0; }}
        tr:hover {{ background: #f1f5f9; }}
        .video-section {{ margin: 2.5rem 0; }}
        .video-container {{ position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; border-radius: 12px; box-shadow: 0 4px 20px rgba(0,0,0,0.12); }}
        .video-container iframe {{ position: absolute; top: 0; left: 0; width: 100%; height: 100%; border: none; }}
        .faq-item {{ background: var(--card); border: 1px solid #e2e8f0; border-radius: 10px; padding: 1.25rem; margin: 1rem 0; box-shadow: 0 2px 8px rgba(0,0,0,0.04); }}
        .faq-q {{ color: var(--primary); font-size: 1.1rem; margin-bottom: 0.5rem; }}
        .faq-a {{ color: #475569; }}
        .cta {{ background: linear-gradient(135deg, var(--primary), #7c3aed); color: white; padding: 2rem; border-radius: 16px; text-align: center; margin: 3rem 0; }}
        .cta a {{ color: var(--accent); font-weight: bold; text-decoration: none; }}
        .footer {{ text-align: center; padding: 2rem 0; color: #94a3b8; border-top: 1px solid #e2e8f0; margin-top: 3rem; font-size: 0.85rem; }}
        .footer a {{ color: var(--primary); text-decoration: none; }}
        @media (max-width: 640px) {{
            body {{ padding: 1rem; }}
            h1 {{ font-size: 1.6rem; }}
        }}
    </style>
</head>
<body>
    <article itemscope itemtype="https://schema.org/TechArticle">
        <h1 itemprop="headline">{title}</h1>
        <div class="meta">
            <span>üìÖ <time itemprop="datePublished" datetime="{date_str}">{date_display}</time></span>
            <span>üìÅ {domain.replace('_', ' ').title()}</span>
            <span>üè¢ <span itemprop="author">Clisonix AI</span></span>
        </div>
        <div class="tags">
            {''.join([f'<span class="tag">{tag}</span>' for tag in tags])}
        </div>
        <div class="content" itemprop="articleBody">
            {html_body}
        </div>
        {video_html}
    </article>
    <div class="cta">
        <h3>üöÄ Ready to explore Clisonix?</h3>
        <p>Visit <a href="https://clisonix.com">clisonix.com</a> |
           <a href="https://github.com/Web8kameleon-hub/clisonix.com">GitHub</a> |
           <a href="mailto:info@clisonix.com">Contact Us</a></p>
    </div>
    <footer class="footer">
        <p>¬© {datetime.now().year}
            <a href="https://clisonix.com">Clisonix</a>
            - ABA GmbH | Advancing Healthcare
            Through Intelligent Signal Processing</p>
    </footer>
</body>
</html>"""

    def _update_index(self, title: str, filename: str, domain: str, tags: List[str]) -> None:
        """Update the articles index JSON"""
        index_path = self.output_dir / "index.json"

        try:
            if index_path.exists():
                index = json.loads(index_path.read_text())
            else:
                index = {"articles": [], "last_updated": None, "total_count": 0}

            index["articles"].insert(0, {
                "title": title,
                "filename": filename,
                "domain": domain,
                "tags": tags,
                "published_at": datetime.now(timezone.utc).isoformat()
            })

            # Keep last 1000 articles in index
            index["articles"] = index["articles"][:1000]
            index["total_count"] = len(index["articles"])
            index["last_updated"] = datetime.now(timezone.utc).isoformat()

            index_path.write_text(json.dumps(index, indent=2))

        except Exception as e:
            logger.error(f"Failed to update index: {e}")


class GitHubPagesPublisher:
    """Publish to GitHub Pages via Git push"""

    def __init__(self, repo_path: str = "/app/published",
                 repo_url: Optional[str] = None,
                 branch: str = "main"):
        self.repo_path = Path(repo_path)
        self.repo_url = repo_url or os.environ.get("GITHUB_PAGES_REPO")
        self.branch = branch
        self._git_configured = False

    async def configure_git(self) -> bool:
        """Configure git for pushing"""
        if self._git_configured:
            return True

        try:
            import subprocess

            # Check if git repo exists
            git_dir = self.repo_path / ".git"
            if not git_dir.exists():
                # Initialize repo
                subprocess.run(["git", "init"], cwd=self.repo_path, check=True)

                if self.repo_url:
                    subprocess.run(
                        ["git", "remote", "add", "origin", self.repo_url],
                        cwd=self.repo_path, check=True
                    )

            # Configure user
            subprocess.run(
                ["git", "config", "user.email", "ai@clisonix.com"],
                cwd=self.repo_path, check=True
            )
            subprocess.run(
                ["git", "config", "user.name", "Clisonix AI"],
                cwd=self.repo_path, check=True
            )

            self._git_configured = True
            return True

        except Exception as e:
            logger.error(f"Git configuration failed: {e}")
            return False

    async def publish(self, commit_message: Optional[str] = None) -> Dict[str, Any]:
        """Commit and push all new articles to GitHub"""
        if not self.repo_url:
            return {
                "success": False,
                "error": "No GitHub repo configured. Set GITHUB_PAGES_REPO env var",
                "platform": "github_pages"
            }

        try:
            import subprocess

            if not await self.configure_git():
                return {"success": False, "error": "Git configuration failed", "platform": "github_pages"}

            # Add all changes
            subprocess.run(["git", "add", "-A"], cwd=self.repo_path, check=True)

            # Check if there are changes
            result = subprocess.run(
                ["git", "status", "--porcelain"],
                cwd=self.repo_path, capture_output=True, text=True
            )

            if not result.stdout.strip():
                return {
                    "success": True,
                    "message": "No new changes to publish",
                    "platform": "github_pages"
                }

            # Commit
            msg = commit_message or f"üìù Auto-publish: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M')}"
            subprocess.run(["git", "commit", "-m", msg], cwd=self.repo_path, check=True)

            # Push
            subprocess.run(
                ["git", "push", "-u", "origin", self.branch],
                cwd=self.repo_path, check=True
            )

            logger.info(f"üöÄ Pushed to GitHub Pages: {self.repo_url}")

            return {
                "success": True,
                "platform": "github_pages",
                "repo": self.repo_url,
                "branch": self.branch,
                "published_at": datetime.now(timezone.utc).isoformat()
            }

        except subprocess.CalledProcessError as e:
            return {"success": False, "error": f"Git error: {e}", "platform": "github_pages"}
        except Exception as e:
            return {"success": False, "error": str(e), "platform": "github_pages"}


class LinkedInPublisher:
    """Publish to LinkedIn"""

    def __init__(self, access_token: Optional[str]):
        self.access_token = access_token
        self.api_url = "https://api.linkedin.com/v2"

    async def publish(self, content: str, title: str) -> Dict[str, Any]:
        """Publish post to LinkedIn"""
        if not self.access_token:
            return {"success": False, "error": "No LinkedIn token configured", "platform": "linkedin"}

        try:
            if httpx:
                async with httpx.AsyncClient() as client:
                    # Get user URN
                    me_response = await client.get(
                        f"{self.api_url}/userinfo",
                        headers={"Authorization": f"Bearer {self.access_token}"}
                    )

                    if me_response.status_code != 200:
                        return {"success": False, "error": "Failed to get user info", "platform": "linkedin"}

                    user_sub = me_response.json().get("sub")

                    # Create post
                    post_data = {
                        "author": f"urn:li:person:{user_sub}",
                        "lifecycleState": "PUBLISHED",
                        "specificContent": {
                            "com.linkedin.ugc.ShareContent": {
                                "shareCommentary": {"text": content[:3000]},
                                "shareMediaCategory": "NONE"
                            }
                        },
                        "visibility": {"com.linkedin.ugc.MemberNetworkVisibility": "PUBLIC"}
                    }

                    response = await client.post(
                        f"{self.api_url}/ugcPosts",
                        headers={
                            "Authorization": f"Bearer {self.access_token}",
                            "Content-Type": "application/json",
                            "X-Restli-Protocol-Version": "2.0.0"
                        },
                        json=post_data
                    )

                    if response.status_code in [200, 201]:
                        post_id = response.headers.get("x-restli-id", "unknown")
                        return {
                            "success": True,
                            "platform": "linkedin",
                            "post_id": post_id,
                            "url": f"https://www.linkedin.com/feed/update/{post_id}",
                            "published_at": datetime.now(timezone.utc).isoformat()
                        }
                    else:
                        return {"success": False, "error": response.text, "platform": "linkedin"}

        except Exception as e:
            return {"success": False, "error": str(e), "platform": "linkedin"}

        return {"success": False, "error": "httpx not available", "platform": "linkedin"}


class MediumPublisher:
    """Publish to Medium"""

    def __init__(self, token: Optional[str]):
        self.token = token
        self.api_url = "https://api.medium.com/v1"

    async def publish(self, content: str, title: str, tags: Optional[List[str]] = None) -> Dict[str, Any]:
        """Publish article to Medium"""
        if not self.token:
            return {"success": False, "error": "No Medium token configured", "platform": "medium"}

        actual_tags = tags if tags else ["Technology", "AI", "Healthcare"]

        try:
            if httpx:
                async with httpx.AsyncClient() as client:
                    # Get user ID
                    me_response = await client.get(
                        f"{self.api_url}/me",
                        headers={"Authorization": f"Bearer {self.token}"}
                    )

                    if me_response.status_code != 200:
                        return {"success": False, "error": "Failed to get user info", "platform": "medium"}

                    user_id = me_response.json().get("data", {}).get("id")

                    # Create post
                    post_data = {
                        "title": title,
                        "contentFormat": "markdown",
                        "content": content,
                        "tags": actual_tags[:5],  # Medium allows max 5 tags
                        "publishStatus": "public"
                    }

                    response = await client.post(
                        f"{self.api_url}/users/{user_id}/posts",
                        headers={
                            "Authorization": f"Bearer {self.token}",
                            "Content-Type": "application/json"
                        },
                        json=post_data
                    )

                    if response.status_code in [200, 201]:
                        data = response.json().get("data", {})
                        return {
                            "success": True,
                            "platform": "medium",
                            "post_id": data.get("id"),
                            "url": data.get("url"),
                            "published_at": datetime.now(timezone.utc).isoformat()
                        }
                    else:
                        return {"success": False, "error": response.text, "platform": "medium"}

        except Exception as e:
            return {"success": False, "error": str(e), "platform": "medium"}

        return {"success": False, "error": "httpx not available", "platform": "medium"}


class DevToPublisher:
    """Publish to Dev.to"""

    def __init__(self, api_key: Optional[str]):
        self.api_key = api_key
        self.api_url = "https://dev.to/api"

    async def publish(self, content: str, title: str, tags: Optional[List[str]] = None) -> Dict[str, Any]:
        """Publish article to Dev.to"""
        if not self.api_key:
            return {"success": False, "error": "No Dev.to API key configured", "platform": "devto"}

        actual_tags = tags if tags else ["healthtech", "ai", "programming", "machinelearning"]

        try:
            if httpx:
                async with httpx.AsyncClient() as client:
                    post_data = {
                        "article": {
                            "title": title,
                            "body_markdown": content,
                            "published": True,
                            "tags": actual_tags[:4]  # Dev.to allows max 4 tags
                        }
                    }

                    response = await client.post(
                        f"{self.api_url}/articles",
                        headers={
                            "api-key": self.api_key,
                            "Content-Type": "application/json"
                        },
                        json=post_data
                    )

                    if response.status_code in [200, 201]:
                        data = response.json()
                        return {
                            "success": True,
                            "platform": "devto",
                            "post_id": data.get("id"),
                            "url": data.get("url"),
                            "published_at": datetime.now(timezone.utc).isoformat()
                        }
                    else:
                        return {"success": False, "error": response.text, "platform": "devto"}

        except Exception as e:
            return {"success": False, "error": str(e), "platform": "devto"}

        return {"success": False, "error": "httpx not available", "platform": "devto"}


class TwitterPublisher:
    """Publish to Twitter/X using OAuth 1.0a (via tweepy)"""

    def __init__(self, api_key: str, api_secret: str, access_token: str, access_secret: str):
        self.api_key = api_key
        self.api_secret = api_secret
        self.access_token = access_token
        self.access_secret = access_secret

    async def publish(self, content: str) -> Dict[str, Any]:
        """Publish tweet via Twitter API v2 with OAuth 1.0a"""
        if not all([self.api_key, self.api_secret, self.access_token, self.access_secret]):
            return {"success": False, "error": "Twitter credentials not configured", "platform": "twitter"}

        try:
            import tweepy
            client = tweepy.Client(
                consumer_key=self.api_key,
                consumer_secret=self.api_secret,
                access_token=self.access_token,
                access_token_secret=self.access_secret
            )
            # Twitter limit = 280 chars
            tweet_text = content[:277] + "..." if len(content) > 280 else content
            response = client.create_tweet(text=tweet_text)
            tweet_id = response.data["id"]
            return {
                "success": True,
                "platform": "twitter",
                "url": f"https://x.com/i/status/{tweet_id}",
                "tweet_id": tweet_id
            }
        except ImportError:
            return {"success": False, "error": "tweepy not installed", "platform": "twitter"}
        except Exception as e:
            return {"success": False, "error": str(e), "platform": "twitter"}


class RedditPublisher:
    """Publish to Reddit via OAuth2 (script app)"""

    def __init__(self, client_id: str, client_secret: str, username: str, password: str, subreddit: str = "clisonix"):
        self.client_id = client_id
        self.client_secret = client_secret
        self.username = username
        self.password = password
        self.subreddit = subreddit
        self.token_url = "https://www.reddit.com/api/v1/access_token"
        self.api_url = "https://oauth.reddit.com"
        self.user_agent = "Clisonix-AutoPublisher/1.0 (by /u/{})".format(username or "clisonix")

    async def _get_token(self) -> Optional[str]:
        """Get Reddit OAuth2 token via password grant"""
        try:
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    self.token_url,
                    auth=(self.client_id, self.client_secret),
                    data={
                        "grant_type": "password",
                        "username": self.username,
                        "password": self.password
                    },
                    headers={"User-Agent": self.user_agent}
                )
                if response.status_code == 200:
                    return response.json().get("access_token")
                logger.warning(f"Reddit token error: {response.status_code} {response.text}")
                return None
        except Exception as e:
            logger.error(f"Reddit auth failed: {e}")
            return None

    async def publish(self, title: str, body: str, flair: str = "") -> Dict[str, Any]:
        """Submit a text post to the configured subreddit"""
        if not all([self.client_id, self.client_secret, self.username, self.password]):
            return {"success": False, "error": "Reddit credentials not configured", "platform": "reddit"}

        token = await self._get_token()
        if not token:
            return {"success": False, "error": "Failed to get Reddit access token", "platform": "reddit"}

        try:
            async with httpx.AsyncClient() as client:
                headers = {
                    "Authorization": f"Bearer {token}",
                    "User-Agent": self.user_agent
                }
                payload = {
                    "sr": self.subreddit,
                    "kind": "self",
                    "title": title[:300],
                    "text": body[:40000],
                    "resubmit": True
                }
                if flair:
                    payload["flair_text"] = flair

                response = await client.post(
                    f"{self.api_url}/api/submit",
                    data=payload,
                    headers=headers
                )

                if response.status_code == 200:
                    data = response.json()
                    if data.get("success") or not data.get("json", {}).get("errors"):
                        post_url = (
                            data.get("json", {}).get("data", {}).get("url")
                            or f"https://reddit.com/r/{self.subreddit}/new"
                        )
                        return {
                            "success": True,
                            "platform": "reddit",
                            "url": post_url,
                            "subreddit": self.subreddit
                        }
                    errors = data.get("json", {}).get("errors", [])
                    return {"success": False, "error": f"Reddit errors: {errors}", "platform": "reddit"}

                err = (
                    f"Reddit HTTP {response.status_code}:"
                    f" {response.text[:200]}"
                )
                return {
                    "success": False,
                    "error": err,
                    "platform": "reddit"
                }

        except Exception as e:
            return {"success": False, "error": str(e), "platform": "reddit"}


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# AUTO PUBLISHER ENGINE
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class AutoPublisher:
    """Main auto-publishing engine"""

    def __init__(self, config: Optional[AutoPublishConfig] = None):
        self.config = config or AutoPublishConfig()
        self.topic_generator = TopicGenerator(self.config)
        self.content_generator = ContentGenerator(self.config)

        # Initialize publishers - LOCAL is ALWAYS enabled (no API needed)
        self.local = LocalPublisher(self.config.output_dir)
        self.github_pages = GitHubPagesPublisher(
            self.config.output_dir,
            os.environ.get("GITHUB_PAGES_REPO")
        )

        # External platform publishers (need API keys)
        self.linkedin = LinkedInPublisher(self.config.linkedin_token)
        self.medium = MediumPublisher(self.config.medium_token)
        self.devto = DevToPublisher(self.config.devto_api_key)
        self.twitter = TwitterPublisher(
            self.config.twitter_api_key or "",
            self.config.twitter_api_secret or "",
            self.config.twitter_access_token or "",
            self.config.twitter_access_secret or ""
        )
        self.reddit = RedditPublisher(
            self.config.reddit_client_id or "",
            self.config.reddit_client_secret or "",
            self.config.reddit_username or "",
            self.config.reddit_password or "",
            self.config.reddit_subreddit
        )

        self._running = False
        self._stats: Dict[str, Any] = {
            "total_generated": 0,
            "total_published": 0,
            "locally_saved": 0,
            "by_platform": {},
            "errors": [],
            "started_at": None
        }

    async def publish_cycle(self) -> Dict[str, Any]:
        """Execute one full publish cycle"""
        cycle_id = hashlib.md5(datetime.now().isoformat().encode()).hexdigest()[:8]
        logger.info(f"üîÑ Starting publish cycle {cycle_id}")

        results: Dict[str, Any] = {
            "cycle_id": cycle_id,
            "started_at": datetime.now(timezone.utc).isoformat(),
            "topic": None,
            "article": None,
            "publications": []
        }

        try:
            # 1. Generate topic (50% template, 50% LLM)
            if random.random() < 0.5:
                topic_result = self.topic_generator.generate_from_template()
            else:
                topic_result = await self.topic_generator.generate_with_llm()

            results["topic"] = topic_result
            logger.info(f"üìù Topic: {topic_result['topic']}")

            # 2. Generate article
            article = await self.content_generator.generate_article(
                topic_result["topic"],
                topic_result["domain"]
            )

            if not article.get("success"):
                logger.error(f"‚ùå Article generation failed: {article.get('error')}")
                results["error"] = "Article generation failed"
                return results

            results["article"] = {
                "title": article["title"],
                "word_count": article["word_count"],
                "domain": article["domain"]
            }
            self._stats["total_generated"] += 1
            logger.info(f"‚úÖ Article generated: {article['word_count']} words")

            # 3. Generate social variants
            variants = self.content_generator.generate_social_variants(article)

            # 4. ALWAYS save locally first (no API needed, always works)
            tags_str = variants.get("devto_tags", "healthtech,ai,programming")
            tags_list = tags_str.split(",") if isinstance(tags_str, str) else ["healthtech", "ai"]

            local_result = await self.local.publish(
                article["content"],
                article["title"],
                article["domain"],
                tags_list,
                video=article.get("video"),
                images=article.get("images")
            )
            results["publications"].append(local_result)

            if local_result.get("success"):
                self._stats["locally_saved"] += 1
                self._stats["total_published"] += 1
                self._stats["by_platform"]["local"] = self._stats["by_platform"].get("local", 0) + 1
                logger.info(f"üìÅ Saved locally: {local_result.get('files', {}).get('jekyll', 'N/A')}")

            # 5. Publish to external platforms (if configured)
            publish_tasks = []

            if self.config.linkedin_enabled:
                publish_tasks.append(("linkedin", self.linkedin.publish(
                    variants["linkedin"], article["title"]
                )))

            if self.config.medium_enabled:
                publish_tasks.append(("medium", self.medium.publish(
                    article["content"], article["title"], tags_list
                )))

            if self.config.devto_enabled:
                publish_tasks.append(("devto", self.devto.publish(
                    article["content"], article["title"], tags_list
                )))

            if self.config.twitter_enabled:
                publish_tasks.append(("twitter", self.twitter.publish(
                    variants["twitter"]
                )))

            if self.config.reddit_enabled:
                publish_tasks.append(("reddit", self.reddit.publish(
                    variants["reddit_title"], variants["reddit_body"]
                )))

            # Execute all publish tasks
            for platform, task in publish_tasks:
                try:
                    result = await task
                    results["publications"].append(result)

                    if result.get("success"):
                        self._stats["total_published"] += 1
                        self._stats["by_platform"][platform] = self._stats["by_platform"].get(platform, 0) + 1
                        logger.info(f"‚úÖ Published to {platform}: {result.get('url', 'N/A')}")
                    else:
                        logger.warning(f"‚ö†Ô∏è Failed to publish to {platform}: {result.get('error')}")

                except Exception as e:
                    error_result = {"success": False, "platform": platform, "error": str(e)}
                    results["publications"].append(error_result)
                    logger.error(f"‚ùå Error publishing to {platform}: {e}")

            # 6. Save article metadata
            await self._save_article(article, results)

        except Exception as e:
            logger.error(f"‚ùå Cycle error: {e}")
            results["error"] = str(e)
            self._stats["errors"].append({
                "cycle_id": cycle_id,
                "error": str(e),
                "timestamp": datetime.now(timezone.utc).isoformat()
            })

        results["completed_at"] = datetime.now(timezone.utc).isoformat()
        return results

    async def _save_article(self, article: Dict[str, Any], results: Dict[str, Any]):
        """Save article to local storage"""
        try:
            output_dir = Path(self.config.output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)

            date_str = datetime.now().strftime("%Y-%m-%d")
            article_id = hashlib.md5(article["title"].encode()).hexdigest()[:8]

            # Save markdown
            md_path = output_dir / f"{date_str}_{article_id}.md"
            with open(md_path, "w", encoding="utf-8") as f:
                f.write(f"# {article['title']}\n\n")
                f.write(f"*Domain: {article['domain']} | Generated: {article['generated_at']}*\n\n")
                f.write(article["content"])

            # Save metadata
            meta_path = output_dir / f"{date_str}_{article_id}_meta.json"
            with open(meta_path, "w", encoding="utf-8") as f:
                json.dump(results, f, indent=2)

            logger.info(f"üíæ Saved to {md_path}")

            # üîÑ Auto-sync to GitHub after each save
            try:
                from blog_sync import get_blog_sync
                sync = get_blog_sync()
                sync_result = await sync.sync(push=True)
                if sync_result.get("status") == "success":
                    synced = sync_result.get("results", {}).get("synced_files", [])
                    if synced:
                        logger.info(f"üöÄ Synced {len(synced)} files to GitHub")
                else:
                    logger.warning(f"‚ö†Ô∏è Sync failed: {sync_result.get('error')}")
            except Exception as sync_error:
                logger.warning(f"‚ö†Ô∏è Blog sync skipped: {sync_error}")

        except Exception as e:
            logger.error(f"Failed to save article: {e}")

    async def run_continuous(self):
        """Run continuous publishing loop"""
        self._running = True
        self._stats["started_at"] = datetime.now(timezone.utc).isoformat()

        logger.info("üöÄ Starting continuous auto-publisher...")
        logger.info(f"   Target: {self.config.docs_per_day_target} docs/day")
        logger.info(f"   Interval: {self.config.min_interval_seconds}-{self.config.max_interval_seconds}s")

        while self._running:
            try:
                # Execute publish cycle
                await self.publish_cycle()

                # Random interval for natural distribution
                interval = random.randint(
                    self.config.min_interval_seconds,
                    self.config.max_interval_seconds
                )

                logger.info(f"‚è≥ Next cycle in {interval}s ({interval / 60:.1f} min)")
                await asyncio.sleep(interval)

            except Exception as e:
                logger.error(f"Continuous loop error: {e}")
                await asyncio.sleep(60)  # Wait 1 min on error

        logger.info("üõë Auto-publisher stopped")

    def stop(self):
        """Stop the continuous loop"""
        self._running = False

    def get_stats(self) -> Dict[str, Any]:
        """Get publishing statistics"""
        return {
            **self._stats,
            "topics_generated": len(self.topic_generator.generated_topics),
            "recent_topics": self.topic_generator.generated_topics[-10:],
            "is_running": self._running
        }


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# FASTAPI INTEGRATION
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# Global instance
_auto_publisher: Optional[AutoPublisher] = None


def get_auto_publisher() -> AutoPublisher:
    """Get or create auto publisher instance"""
    global _auto_publisher
    if _auto_publisher is None:
        _auto_publisher = AutoPublisher()
    return _auto_publisher


async def start_auto_publisher():
    """Start the auto-publisher (call from FastAPI startup)"""
    publisher = get_auto_publisher()
    asyncio.create_task(publisher.run_continuous())


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# CLI
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

if __name__ == "__main__":
    import sys

    async def main():
        publisher = AutoPublisher()

        if len(sys.argv) > 1 and sys.argv[1] == "once":
            # Single cycle
            result = await publisher.publish_cycle()
            print(json.dumps(result, indent=2))
        else:
            # Continuous mode
            await publisher.run_continuous()

    asyncio.run(main())
